{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Star Wars Text Generation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOmwKHijLOdg"
      },
      "source": [
        "# **Generation of Star Wars Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biV1z0koaDHT"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGscdaCtpmbV"
      },
      "source": [
        "### Install HuggingFace Transfomers library.\r\n",
        "\r\n",
        "I use a slightly older version of huggingface's transformer library to guarantee the compatibility with the fine tuning script. \r\n",
        "\r\n",
        "The fine tuning can be performed using the run_language_modelling.py of the library, or with the one from the github repository, which I have modified to perform more evaluation during training. \r\n",
        "\r\n",
        "To run the fine tuning, it is necessary to load the training data in the colab environnement. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicio9FLPv5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b02b3c5-8631-4704-bd6c-ce540b954244"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 64814 (delta 13), reused 2 (delta 0), pack-reused 64779\u001b[K\n",
            "Receiving objects: 100% (64814/64814), 48.73 MiB | 29.56 MiB/s, done.\n",
            "Resolving deltas: 100% (45936/45936), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNjHCx6xLMWW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weh0BoPfk1zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f009a00b-8bae-4116-a1ab-527f7c108178"
      },
      "source": [
        "# imports and utilities \n",
        "import os\n",
        "os.chdir('/content/transformers')\n",
        "\n",
        "# Use language modeling version as of April 21st.\n",
        "!git checkout b1ff0b2ae7d368b7db3a8a8472a29cc195d278d8\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt\n",
        "\n",
        "os.chdir('/content/transformers/examples')\n",
        "\n",
        "\n",
        "import torch\n",
        "import run_language_modeling\n",
        "import run_generation\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelWithLMHead\n",
        "from transformers import GPT2Config\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "def to_object(item):\n",
        "        \"\"\"\n",
        "        Convert a dictionary to an object (recursive).\n",
        "        \"\"\"\n",
        "        def convert(item): \n",
        "            if isinstance(item, dict):\n",
        "                return type('jo', (), {k: convert(v) for k, v in item.items()})\n",
        "            if isinstance(item, list):\n",
        "                def yield_convert(item):\n",
        "                    for index, value in enumerate(item):\n",
        "                        yield convert(value)\n",
        "                return list(yield_convert(item))\n",
        "            else:\n",
        "                return item\n",
        "\n",
        "        return convert(item)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: checking out 'b1ff0b2ae7d368b7db3a8a8472a29cc195d278d8'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at b1ff0b2a Fix bug in examples: double wrap into DataParallel during eval\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Collecting tokenizers==0.7.0rc7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/f6/c910fd504ded3072c5c810a5c1572c41e7cec5a5f7879d44be533ab881a4/tokenizers-0.7.0rc7-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 13.1MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/c8/b5aac643697038ef6eb8c11c73b9ee9c2dc8cb2bc95cda2d4ee656167644/boto3-1.17.17-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 61.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 55.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 54.3MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/fb/7ea265e28306dde068c74e6792affd4df43e51784384829c69142042ad56/botocore-1.20.17-py2.py3-none-any.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 45.1MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.17->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.8.0-cp37-none-any.whl size=570763 sha256=b445dad44238fe01503eeb0ce1c97ebd4a69e0e8cf38cebadd7d7945f21644fb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m1tzqz8l/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=a4c8b71deac4491ced332b0879e4773078f1adc4fd2d57b10ed8a25c412f1942\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.17 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tokenizers, jmespath, botocore, s3transfer, boto3, sentencepiece, sacremoses, transformers\n",
            "Successfully installed boto3-1.17.17 botocore-1.20.17 jmespath-0.10.0 s3transfer-0.3.4 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.7.0rc7 transformers-2.8.0\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 17.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 2)) (2.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 5)) (5.4.8)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.1MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (from -r ./examples/requirements.txt (line 8)) (4.0.1)\n",
            "Collecting pytorch-lightning==0.7.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/53/0549dd9c44c90e96d217592e094e9c53ef39ae2fed0c5cdb7e57aca65af6/pytorch_lightning-0.7.3-py3-none-any.whl (203kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 15.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.32.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.27.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (53.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.36.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.0.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/82/22/e684c9e2e59b561dbe36538852e81849122c666c423448e3a5c99362c228/portalocker-2.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score->-r ./examples/requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.3.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (4.41.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (20.3.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.1.5)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (5.1.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.28.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==0.7.3->-r ./examples/requirements.txt (line 9)) (1.7.1+cu101)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->-r ./examples/requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.52.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->pytorch-lightning==0.7.3->-r ./examples/requirements.txt (line 9)) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=f600dc712a2e632a403cc0b19d848031fdaad4139debab29d8e14425a611c25b\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "\u001b[31mERROR: pytorch-lightning 0.7.3 has requirement future>=0.17.1, but you'll have future 0.16.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboardX, seqeval, portalocker, sacrebleu, rouge-score, pytorch-lightning\n",
            "Successfully installed portalocker-2.2.1 pytorch-lightning-0.7.3 rouge-score-0.0.4 sacrebleu-1.5.0 seqeval-1.2.2 tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX2LEl-uwMXp"
      },
      "source": [
        "\r\n",
        "### Mount Google Drive\r\n",
        "\r\n",
        "The Google Drive is used to save the model during and after training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ6FFHiMMP0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ccaa3a-1424-4397-a803-bcec70343ea0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzFhwDSqOg3"
      },
      "source": [
        "# Launch fine-tuninng\n",
        "\n",
        "\n",
        "Fine tuning is performed by calling the script from command line. The following hyperparameters can be modified : \n",
        "\n",
        "* `--num_train_epochs`: The number of times to iterate over the train set. \n",
        "* `--block_size`: Training text is truncated into blocks of this length.\n",
        "* `--gradient_accumulation_steps`: Update the model weights every this many steps. Set this to >1 when the batch size is very small to improve training stability.\n",
        "* `--output_dir`: This is the where checkpoints will get saved. \n",
        "* `--model_name_or_path` The path to the model weights to use when starting fine-tuning. \n",
        "* `--learning_rate` : learning rate of the Adam Optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33GutF1QVEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c387d8-c20a-43b8-da50-8d6831a8aac4"
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models/star_wars_final' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2-medium \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=3.0 \\\n",
        "    --do_train \\\n",
        "    --evaluate_during_training \\\n",
        "    --logging_steps=500 \\\n",
        "    --save_steps=500 \\\n",
        "    --train_data_file=/content/star_wars_train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=/content/star_wars_valid.txt \\\n",
        "    --per_gpu_train_batch_size=2 \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=512 \\\n",
        "    --gradient_accumulation_steps=5 \\\n",
        "    --learning_rate=1e-4 "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-02-27 21:09:12.524875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "02/27/2021 21:09:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/27/2021 21:09:14 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json from cache at /root/.cache/torch/transformers/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.250d6dc755ccb17d19c7c1a7677636683aa35f0f6cb5461b3c0587bc091551a0\n",
            "02/27/2021 21:09:14 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/27/2021 21:09:14 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json from cache at /root/.cache/torch/transformers/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.250d6dc755ccb17d19c7c1a7677636683aa35f0f6cb5461b3c0587bc091551a0\n",
            "02/27/2021 21:09:14 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/27/2021 21:09:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json from cache at /root/.cache/torch/transformers/f20f05d3ae37c4e3cd56764d48e566ea5adeba153dcee6eb82a18822c9c731ec.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "02/27/2021 21:09:14 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-merges.txt from cache at /root/.cache/torch/transformers/6d882670c55563617571fe0c97df88626fb5033927b40fc18a8acf98dafd4946.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "02/27/2021 21:09:14 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin from cache at /root/.cache/torch/transformers/4b337a4f3b7d3e1518f799e238af607498c02938a3390152aaec7d4dabca5a02.8769029be4f66a5ae1055eefdd1d11621b901d510654266b8681719fff492d6e\n",
            "02/27/2021 21:10:08 - INFO - transformers.modeling_utils -   Weights of GPT2LMHeadModel not initialized from pretrained model: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias', 'lm_head.weight']\n",
            "02/27/2021 21:10:22 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/star_wars_valid.txt', evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=5, learning_rate=0.0001, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2-medium', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='/content/drive/My Drive/finetuned_models/star_wars_final', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=2, per_gpu_train_batch_size=2, save_steps=500, save_total_limit=5, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/star_wars_train.txt', warmup_steps=0, weight_decay=0.0)\n",
            "02/27/2021 21:10:22 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "02/27/2021 21:10:24 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_512_star_wars_train.txt\n",
            "02/27/2021 21:10:24 - INFO - __main__ -   ***** Running training *****\n",
            "02/27/2021 21:10:24 - INFO - __main__ -     Num examples = 722\n",
            "02/27/2021 21:10:24 - INFO - __main__ -     Num Epochs = 3\n",
            "02/27/2021 21:10:24 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "02/27/2021 21:10:24 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "02/27/2021 21:10:24 - INFO - __main__ -     Gradient Accumulation steps = 5\n",
            "02/27/2021 21:10:24 - INFO - __main__ -     Total optimization steps = 216\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/361 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/361 [00:01<10:30,  1.75s/it]\u001b[A\n",
            "Iteration:   1% 2/361 [00:02<08:40,  1.45s/it]\u001b[A\n",
            "Iteration:   1% 3/361 [00:03<07:22,  1.24s/it]\u001b[A\n",
            "Iteration:   1% 4/361 [00:03<06:28,  1.09s/it]\u001b[A/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
            "\n",
            "Iteration:   1% 5/361 [00:04<06:09,  1.04s/it]\u001b[A\n",
            "Iteration:   2% 6/361 [00:05<05:37,  1.05it/s]\u001b[A\n",
            "Iteration:   2% 7/361 [00:06<05:15,  1.12it/s]\u001b[A\n",
            "Iteration:   2% 8/361 [00:07<05:01,  1.17it/s]\u001b[A\n",
            "Iteration:   2% 9/361 [00:07<04:51,  1.21it/s]\u001b[A\n",
            "Iteration:   3% 10/361 [00:08<04:58,  1.18it/s]\u001b[A\n",
            "Iteration:   3% 11/361 [00:09<04:46,  1.22it/s]\u001b[A\n",
            "Iteration:   3% 12/361 [00:10<04:38,  1.25it/s]\u001b[A\n",
            "Iteration:   4% 13/361 [00:11<04:34,  1.27it/s]\u001b[A\n",
            "Iteration:   4% 14/361 [00:11<04:30,  1.28it/s]\u001b[A\n",
            "Iteration:   4% 15/361 [00:12<04:41,  1.23it/s]\u001b[A\n",
            "Iteration:   4% 16/361 [00:13<04:33,  1.26it/s]\u001b[A\n",
            "Iteration:   5% 17/361 [00:14<04:30,  1.27it/s]\u001b[A\n",
            "Iteration:   5% 18/361 [00:15<04:28,  1.28it/s]\u001b[A\n",
            "Iteration:   5% 19/361 [00:15<04:26,  1.28it/s]\u001b[A\n",
            "Iteration:   6% 20/361 [00:16<04:38,  1.22it/s]\u001b[A\n",
            "Iteration:   6% 21/361 [00:17<04:32,  1.25it/s]\u001b[A\n",
            "Iteration:   6% 22/361 [00:18<04:28,  1.26it/s]\u001b[A\n",
            "Iteration:   6% 23/361 [00:19<04:27,  1.27it/s]\u001b[A\n",
            "Iteration:   7% 24/361 [00:19<04:25,  1.27it/s]\u001b[A\n",
            "Iteration:   7% 25/361 [00:20<04:37,  1.21it/s]\u001b[A\n",
            "Iteration:   7% 26/361 [00:21<04:32,  1.23it/s]\u001b[A\n",
            "Iteration:   7% 27/361 [00:22<04:27,  1.25it/s]\u001b[A\n",
            "Iteration:   8% 28/361 [00:23<04:23,  1.26it/s]\u001b[A\n",
            "Iteration:   8% 29/361 [00:23<04:22,  1.27it/s]\u001b[A\n",
            "Iteration:   8% 30/361 [00:24<04:33,  1.21it/s]\u001b[A\n",
            "Iteration:   9% 31/361 [00:25<04:26,  1.24it/s]\u001b[A\n",
            "Iteration:   9% 32/361 [00:26<04:22,  1.25it/s]\u001b[A\n",
            "Iteration:   9% 33/361 [00:27<04:20,  1.26it/s]\u001b[A\n",
            "Iteration:   9% 34/361 [00:27<04:19,  1.26it/s]\u001b[A\n",
            "Iteration:  10% 35/361 [00:28<04:31,  1.20it/s]\u001b[A\n",
            "Iteration:  10% 36/361 [00:29<04:25,  1.22it/s]\u001b[A\n",
            "Iteration:  10% 37/361 [00:30<04:22,  1.24it/s]\u001b[A\n",
            "Iteration:  11% 38/361 [00:31<04:19,  1.24it/s]\u001b[A\n",
            "Iteration:  11% 39/361 [00:31<04:18,  1.25it/s]\u001b[A\n",
            "Iteration:  11% 40/361 [00:32<04:30,  1.19it/s]\u001b[A\n",
            "Iteration:  11% 41/361 [00:33<04:25,  1.21it/s]\u001b[A\n",
            "Iteration:  12% 42/361 [00:34<04:21,  1.22it/s]\u001b[A\n",
            "Iteration:  12% 43/361 [00:35<04:19,  1.22it/s]\u001b[A\n",
            "Iteration:  12% 44/361 [00:36<04:18,  1.23it/s]\u001b[A\n",
            "Iteration:  12% 45/361 [00:37<04:31,  1.16it/s]\u001b[A\n",
            "Iteration:  13% 46/361 [00:37<04:26,  1.18it/s]\u001b[A\n",
            "Iteration:  13% 47/361 [00:38<04:22,  1.20it/s]\u001b[A\n",
            "Iteration:  13% 48/361 [00:39<04:19,  1.21it/s]\u001b[A\n",
            "Iteration:  14% 49/361 [00:40<04:18,  1.21it/s]\u001b[A\n",
            "Iteration:  14% 50/361 [00:41<04:30,  1.15it/s]\u001b[A\n",
            "Iteration:  14% 51/361 [00:42<04:26,  1.16it/s]\u001b[A\n",
            "Iteration:  14% 52/361 [00:42<04:22,  1.18it/s]\u001b[A\n",
            "Iteration:  15% 53/361 [00:43<04:20,  1.18it/s]\u001b[A\n",
            "Iteration:  15% 54/361 [00:44<04:19,  1.18it/s]\u001b[A\n",
            "Iteration:  15% 55/361 [00:45<04:29,  1.13it/s]\u001b[A\n",
            "Iteration:  16% 56/361 [00:46<04:25,  1.15it/s]\u001b[A\n",
            "Iteration:  16% 57/361 [00:47<04:22,  1.16it/s]\u001b[A\n",
            "Iteration:  16% 58/361 [00:48<04:20,  1.16it/s]\u001b[A\n",
            "Iteration:  16% 59/361 [00:49<04:18,  1.17it/s]\u001b[A\n",
            "Iteration:  17% 60/361 [00:50<04:29,  1.12it/s]\u001b[A\n",
            "Iteration:  17% 61/361 [00:50<04:25,  1.13it/s]\u001b[A\n",
            "Iteration:  17% 62/361 [00:51<04:21,  1.14it/s]\u001b[A\n",
            "Iteration:  17% 63/361 [00:52<04:19,  1.15it/s]\u001b[A\n",
            "Iteration:  18% 64/361 [00:53<04:17,  1.15it/s]\u001b[A\n",
            "Iteration:  18% 65/361 [00:54<04:28,  1.10it/s]\u001b[A\n",
            "Iteration:  18% 66/361 [00:55<04:24,  1.12it/s]\u001b[A\n",
            "Iteration:  19% 67/361 [00:56<04:20,  1.13it/s]\u001b[A\n",
            "Iteration:  19% 68/361 [00:57<04:18,  1.13it/s]\u001b[A\n",
            "Iteration:  19% 69/361 [00:57<04:16,  1.14it/s]\u001b[A\n",
            "Iteration:  19% 70/361 [00:58<04:26,  1.09it/s]\u001b[A\n",
            "Iteration:  20% 71/361 [00:59<04:22,  1.10it/s]\u001b[A\n",
            "Iteration:  20% 72/361 [01:00<04:18,  1.12it/s]\u001b[A\n",
            "Iteration:  20% 73/361 [01:01<04:15,  1.13it/s]\u001b[A\n",
            "Iteration:  20% 74/361 [01:02<04:14,  1.13it/s]\u001b[A\n",
            "Iteration:  21% 75/361 [01:03<04:26,  1.08it/s]\u001b[A\n",
            "Iteration:  21% 76/361 [01:04<04:21,  1.09it/s]\u001b[A\n",
            "Iteration:  21% 77/361 [01:05<04:17,  1.10it/s]\u001b[A\n",
            "Iteration:  22% 78/361 [01:06<04:13,  1.12it/s]\u001b[A\n",
            "Iteration:  22% 79/361 [01:06<04:10,  1.13it/s]\u001b[A\n",
            "Iteration:  22% 80/361 [01:07<04:21,  1.08it/s]\u001b[A\n",
            "Iteration:  22% 81/361 [01:08<04:16,  1.09it/s]\u001b[A\n",
            "Iteration:  23% 82/361 [01:09<04:11,  1.11it/s]\u001b[A\n",
            "Iteration:  23% 83/361 [01:10<04:07,  1.12it/s]\u001b[A\n",
            "Iteration:  23% 84/361 [01:11<04:05,  1.13it/s]\u001b[A\n",
            "Iteration:  24% 85/361 [01:12<04:15,  1.08it/s]\u001b[A\n",
            "Iteration:  24% 86/361 [01:13<04:10,  1.10it/s]\u001b[A\n",
            "Iteration:  24% 87/361 [01:14<04:05,  1.12it/s]\u001b[A\n",
            "Iteration:  24% 88/361 [01:15<04:02,  1.13it/s]\u001b[A\n",
            "Iteration:  25% 89/361 [01:15<03:59,  1.14it/s]\u001b[A\n",
            "Iteration:  25% 90/361 [01:16<04:07,  1.09it/s]\u001b[A\n",
            "Iteration:  25% 91/361 [01:17<04:02,  1.11it/s]\u001b[A\n",
            "Iteration:  25% 92/361 [01:18<03:57,  1.13it/s]\u001b[A\n",
            "Iteration:  26% 93/361 [01:19<03:53,  1.15it/s]\u001b[A\n",
            "Iteration:  26% 94/361 [01:20<03:51,  1.15it/s]\u001b[A\n",
            "Iteration:  26% 95/361 [01:21<04:00,  1.11it/s]\u001b[A\n",
            "Iteration:  27% 96/361 [01:22<03:55,  1.12it/s]\u001b[A\n",
            "Iteration:  27% 97/361 [01:23<03:51,  1.14it/s]\u001b[A\n",
            "Iteration:  27% 98/361 [01:23<03:47,  1.15it/s]\u001b[A\n",
            "Iteration:  27% 99/361 [01:24<03:45,  1.16it/s]\u001b[A\n",
            "Iteration:  28% 100/361 [01:25<03:54,  1.11it/s]\u001b[A\n",
            "Iteration:  28% 101/361 [01:26<03:49,  1.13it/s]\u001b[A\n",
            "Iteration:  28% 102/361 [01:27<03:45,  1.15it/s]\u001b[A\n",
            "Iteration:  29% 103/361 [01:28<03:42,  1.16it/s]\u001b[A\n",
            "Iteration:  29% 104/361 [01:29<03:40,  1.17it/s]\u001b[A\n",
            "Iteration:  29% 105/361 [01:30<03:49,  1.12it/s]\u001b[A\n",
            "Iteration:  29% 106/361 [01:30<03:43,  1.14it/s]\u001b[A\n",
            "Iteration:  30% 107/361 [01:31<03:39,  1.16it/s]\u001b[A\n",
            "Iteration:  30% 108/361 [01:32<03:36,  1.17it/s]\u001b[A\n",
            "Iteration:  30% 109/361 [01:33<03:34,  1.18it/s]\u001b[A\n",
            "Iteration:  30% 110/361 [01:34<03:42,  1.13it/s]\u001b[A\n",
            "Iteration:  31% 111/361 [01:35<03:37,  1.15it/s]\u001b[A\n",
            "Iteration:  31% 112/361 [01:36<03:33,  1.17it/s]\u001b[A\n",
            "Iteration:  31% 113/361 [01:36<03:30,  1.18it/s]\u001b[A\n",
            "Iteration:  32% 114/361 [01:37<03:28,  1.19it/s]\u001b[A\n",
            "Iteration:  32% 115/361 [01:38<03:37,  1.13it/s]\u001b[A\n",
            "Iteration:  32% 116/361 [01:39<03:32,  1.15it/s]\u001b[A\n",
            "Iteration:  32% 117/361 [01:40<03:28,  1.17it/s]\u001b[A\n",
            "Iteration:  33% 118/361 [01:41<03:25,  1.18it/s]\u001b[A\n",
            "Iteration:  33% 119/361 [01:42<03:23,  1.19it/s]\u001b[A\n",
            "Iteration:  33% 120/361 [01:42<03:31,  1.14it/s]\u001b[A\n",
            "Iteration:  34% 121/361 [01:43<03:27,  1.16it/s]\u001b[A\n",
            "Iteration:  34% 122/361 [01:44<03:23,  1.18it/s]\u001b[A\n",
            "Iteration:  34% 123/361 [01:45<03:20,  1.19it/s]\u001b[A\n",
            "Iteration:  34% 124/361 [01:46<03:18,  1.20it/s]\u001b[A\n",
            "Iteration:  35% 125/361 [01:47<03:27,  1.14it/s]\u001b[A\n",
            "Iteration:  35% 126/361 [01:48<03:22,  1.16it/s]\u001b[A\n",
            "Iteration:  35% 127/361 [01:48<03:18,  1.18it/s]\u001b[A\n",
            "Iteration:  35% 128/361 [01:49<03:15,  1.19it/s]\u001b[A\n",
            "Iteration:  36% 129/361 [01:50<03:13,  1.20it/s]\u001b[A\n",
            "Iteration:  36% 130/361 [01:51<03:22,  1.14it/s]\u001b[A\n",
            "Iteration:  36% 131/361 [01:52<03:18,  1.16it/s]\u001b[A\n",
            "Iteration:  37% 132/361 [01:53<03:13,  1.18it/s]\u001b[A\n",
            "Iteration:  37% 133/361 [01:53<03:11,  1.19it/s]\u001b[A\n",
            "Iteration:  37% 134/361 [01:54<03:09,  1.20it/s]\u001b[A\n",
            "Iteration:  37% 135/361 [01:55<03:16,  1.15it/s]\u001b[A\n",
            "Iteration:  38% 136/361 [01:56<03:12,  1.17it/s]\u001b[A\n",
            "Iteration:  38% 137/361 [01:57<03:09,  1.18it/s]\u001b[A\n",
            "Iteration:  38% 138/361 [01:58<03:06,  1.19it/s]\u001b[A\n",
            "Iteration:  39% 139/361 [01:59<03:05,  1.20it/s]\u001b[A\n",
            "Iteration:  39% 140/361 [02:00<03:14,  1.14it/s]\u001b[A\n",
            "Iteration:  39% 141/361 [02:00<03:09,  1.16it/s]\u001b[A\n",
            "Iteration:  39% 142/361 [02:01<03:06,  1.18it/s]\u001b[A\n",
            "Iteration:  40% 143/361 [02:02<03:04,  1.18it/s]\u001b[A\n",
            "Iteration:  40% 144/361 [02:03<03:02,  1.19it/s]\u001b[A\n",
            "Iteration:  40% 145/361 [02:04<03:10,  1.13it/s]\u001b[A\n",
            "Iteration:  40% 146/361 [02:05<03:06,  1.15it/s]\u001b[A\n",
            "Iteration:  41% 147/361 [02:05<03:02,  1.17it/s]\u001b[A\n",
            "Iteration:  41% 148/361 [02:06<03:00,  1.18it/s]\u001b[A\n",
            "Iteration:  41% 149/361 [02:07<02:58,  1.19it/s]\u001b[A\n",
            "Iteration:  42% 150/361 [02:08<03:07,  1.13it/s]\u001b[A\n",
            "Iteration:  42% 151/361 [02:09<03:03,  1.15it/s]\u001b[A\n",
            "Iteration:  42% 152/361 [02:10<02:59,  1.16it/s]\u001b[A\n",
            "Iteration:  42% 153/361 [02:11<02:57,  1.17it/s]\u001b[A\n",
            "Iteration:  43% 154/361 [02:11<02:55,  1.18it/s]\u001b[A\n",
            "Iteration:  43% 155/361 [02:12<03:03,  1.12it/s]\u001b[A\n",
            "Iteration:  43% 156/361 [02:13<02:59,  1.14it/s]\u001b[A\n",
            "Iteration:  43% 157/361 [02:14<02:55,  1.16it/s]\u001b[A\n",
            "Iteration:  44% 158/361 [02:15<02:54,  1.17it/s]\u001b[A\n",
            "Iteration:  44% 159/361 [02:16<02:52,  1.17it/s]\u001b[A\n",
            "Iteration:  44% 160/361 [02:17<03:00,  1.12it/s]\u001b[A\n",
            "Iteration:  45% 161/361 [02:18<02:56,  1.13it/s]\u001b[A\n",
            "Iteration:  45% 162/361 [02:19<02:52,  1.15it/s]\u001b[A\n",
            "Iteration:  45% 163/361 [02:19<02:50,  1.16it/s]\u001b[A\n",
            "Iteration:  45% 164/361 [02:20<02:49,  1.16it/s]\u001b[A\n",
            "Iteration:  46% 165/361 [02:21<02:55,  1.11it/s]\u001b[A\n",
            "Iteration:  46% 166/361 [02:22<02:52,  1.13it/s]\u001b[A\n",
            "Iteration:  46% 167/361 [02:23<02:48,  1.15it/s]\u001b[A\n",
            "Iteration:  47% 168/361 [02:24<02:46,  1.16it/s]\u001b[A\n",
            "Iteration:  47% 169/361 [02:25<02:45,  1.16it/s]\u001b[A\n",
            "Iteration:  47% 170/361 [02:26<02:52,  1.11it/s]\u001b[A\n",
            "Iteration:  47% 171/361 [02:26<02:48,  1.13it/s]\u001b[A\n",
            "Iteration:  48% 172/361 [02:27<02:45,  1.14it/s]\u001b[A\n",
            "Iteration:  48% 173/361 [02:28<02:42,  1.15it/s]\u001b[A\n",
            "Iteration:  48% 174/361 [02:29<02:41,  1.16it/s]\u001b[A\n",
            "Iteration:  48% 175/361 [02:30<02:48,  1.10it/s]\u001b[A\n",
            "Iteration:  49% 176/361 [02:31<02:44,  1.13it/s]\u001b[A\n",
            "Iteration:  49% 177/361 [02:32<02:40,  1.14it/s]\u001b[A\n",
            "Iteration:  49% 178/361 [02:33<02:38,  1.15it/s]\u001b[A\n",
            "Iteration:  50% 179/361 [02:33<02:36,  1.16it/s]\u001b[A\n",
            "Iteration:  50% 180/361 [02:34<02:42,  1.11it/s]\u001b[A\n",
            "Iteration:  50% 181/361 [02:35<02:39,  1.13it/s]\u001b[A\n",
            "Iteration:  50% 182/361 [02:36<02:36,  1.14it/s]\u001b[A\n",
            "Iteration:  51% 183/361 [02:37<02:34,  1.15it/s]\u001b[A\n",
            "Iteration:  51% 184/361 [02:38<02:32,  1.16it/s]\u001b[A\n",
            "Iteration:  51% 185/361 [02:39<02:38,  1.11it/s]\u001b[A\n",
            "Iteration:  52% 186/361 [02:40<02:35,  1.13it/s]\u001b[A\n",
            "Iteration:  52% 187/361 [02:40<02:32,  1.14it/s]\u001b[A\n",
            "Iteration:  52% 188/361 [02:41<02:30,  1.15it/s]\u001b[A\n",
            "Iteration:  52% 189/361 [02:42<02:28,  1.16it/s]\u001b[A\n",
            "Iteration:  53% 190/361 [02:43<02:33,  1.11it/s]\u001b[A\n",
            "Iteration:  53% 191/361 [02:44<02:30,  1.13it/s]\u001b[A\n",
            "Iteration:  53% 192/361 [02:45<02:27,  1.14it/s]\u001b[A\n",
            "Iteration:  53% 193/361 [02:46<02:25,  1.15it/s]\u001b[A\n",
            "Iteration:  54% 194/361 [02:47<02:23,  1.16it/s]\u001b[A\n",
            "Iteration:  54% 195/361 [02:48<02:29,  1.11it/s]\u001b[A\n",
            "Iteration:  54% 196/361 [02:48<02:25,  1.13it/s]\u001b[A\n",
            "Iteration:  55% 197/361 [02:49<02:22,  1.15it/s]\u001b[A\n",
            "Iteration:  55% 198/361 [02:50<02:20,  1.16it/s]\u001b[A\n",
            "Iteration:  55% 199/361 [02:51<02:19,  1.16it/s]\u001b[A\n",
            "Iteration:  55% 200/361 [02:52<02:25,  1.11it/s]\u001b[A\n",
            "Iteration:  56% 201/361 [02:53<02:21,  1.13it/s]\u001b[A\n",
            "Iteration:  56% 202/361 [02:54<02:18,  1.15it/s]\u001b[A\n",
            "Iteration:  56% 203/361 [02:54<02:16,  1.16it/s]\u001b[A\n",
            "Iteration:  57% 204/361 [02:55<02:14,  1.16it/s]\u001b[A\n",
            "Iteration:  57% 205/361 [02:56<02:19,  1.11it/s]\u001b[A\n",
            "Iteration:  57% 206/361 [02:57<02:16,  1.13it/s]\u001b[A\n",
            "Iteration:  57% 207/361 [02:58<02:14,  1.15it/s]\u001b[A\n",
            "Iteration:  58% 208/361 [02:59<02:11,  1.16it/s]\u001b[A\n",
            "Iteration:  58% 209/361 [03:00<02:09,  1.17it/s]\u001b[A\n",
            "Iteration:  58% 210/361 [03:01<02:15,  1.12it/s]\u001b[A\n",
            "Iteration:  58% 211/361 [03:01<02:11,  1.14it/s]\u001b[A\n",
            "Iteration:  59% 212/361 [03:02<02:08,  1.16it/s]\u001b[A\n",
            "Iteration:  59% 213/361 [03:03<02:07,  1.17it/s]\u001b[A\n",
            "Iteration:  59% 214/361 [03:04<02:05,  1.17it/s]\u001b[A\n",
            "Iteration:  60% 215/361 [03:05<02:10,  1.12it/s]\u001b[A\n",
            "Iteration:  60% 216/361 [03:06<02:07,  1.14it/s]\u001b[A\n",
            "Iteration:  60% 217/361 [03:07<02:05,  1.15it/s]\u001b[A\n",
            "Iteration:  60% 218/361 [03:08<02:03,  1.16it/s]\u001b[A\n",
            "Iteration:  61% 219/361 [03:08<02:01,  1.17it/s]\u001b[A\n",
            "Iteration:  61% 220/361 [03:09<02:05,  1.12it/s]\u001b[A\n",
            "Iteration:  61% 221/361 [03:10<02:02,  1.14it/s]\u001b[A\n",
            "Iteration:  61% 222/361 [03:11<02:00,  1.16it/s]\u001b[A\n",
            "Iteration:  62% 223/361 [03:12<01:58,  1.17it/s]\u001b[A\n",
            "Iteration:  62% 224/361 [03:13<01:56,  1.17it/s]\u001b[A\n",
            "Iteration:  62% 225/361 [03:14<02:00,  1.13it/s]\u001b[A\n",
            "Iteration:  63% 226/361 [03:14<01:57,  1.14it/s]\u001b[A\n",
            "Iteration:  63% 227/361 [03:15<01:55,  1.16it/s]\u001b[A\n",
            "Iteration:  63% 228/361 [03:16<01:53,  1.17it/s]\u001b[A\n",
            "Iteration:  63% 229/361 [03:17<01:52,  1.18it/s]\u001b[A\n",
            "Iteration:  64% 230/361 [03:18<01:56,  1.12it/s]\u001b[A\n",
            "Iteration:  64% 231/361 [03:19<01:53,  1.15it/s]\u001b[A\n",
            "Iteration:  64% 232/361 [03:20<01:50,  1.16it/s]\u001b[A\n",
            "Iteration:  65% 233/361 [03:20<01:49,  1.17it/s]\u001b[A\n",
            "Iteration:  65% 234/361 [03:21<01:47,  1.18it/s]\u001b[A\n",
            "Iteration:  65% 235/361 [03:22<01:52,  1.12it/s]\u001b[A\n",
            "Iteration:  65% 236/361 [03:23<01:49,  1.14it/s]\u001b[A\n",
            "Iteration:  66% 237/361 [03:24<01:46,  1.16it/s]\u001b[A\n",
            "Iteration:  66% 238/361 [03:25<01:44,  1.17it/s]\u001b[A\n",
            "Iteration:  66% 239/361 [03:26<01:43,  1.18it/s]\u001b[A\n",
            "Iteration:  66% 240/361 [03:27<01:47,  1.13it/s]\u001b[A\n",
            "Iteration:  67% 241/361 [03:27<01:44,  1.14it/s]\u001b[A\n",
            "Iteration:  67% 242/361 [03:28<01:42,  1.16it/s]\u001b[A\n",
            "Iteration:  67% 243/361 [03:29<01:40,  1.17it/s]\u001b[A\n",
            "Iteration:  68% 244/361 [03:30<01:39,  1.18it/s]\u001b[A\n",
            "Iteration:  68% 245/361 [03:31<01:42,  1.13it/s]\u001b[A\n",
            "Iteration:  68% 246/361 [03:32<01:40,  1.15it/s]\u001b[A\n",
            "Iteration:  68% 247/361 [03:33<01:37,  1.17it/s]\u001b[A\n",
            "Iteration:  69% 248/361 [03:33<01:36,  1.17it/s]\u001b[A\n",
            "Iteration:  69% 249/361 [03:34<01:34,  1.18it/s]\u001b[A\n",
            "Iteration:  69% 250/361 [03:35<01:38,  1.13it/s]\u001b[A\n",
            "Iteration:  70% 251/361 [03:36<01:35,  1.15it/s]\u001b[A\n",
            "Iteration:  70% 252/361 [03:37<01:33,  1.17it/s]\u001b[A\n",
            "Iteration:  70% 253/361 [03:38<01:32,  1.17it/s]\u001b[A\n",
            "Iteration:  70% 254/361 [03:39<01:30,  1.18it/s]\u001b[A\n",
            "Iteration:  71% 255/361 [03:40<01:34,  1.13it/s]\u001b[A\n",
            "Iteration:  71% 256/361 [03:40<01:31,  1.14it/s]\u001b[A\n",
            "Iteration:  71% 257/361 [03:41<01:29,  1.16it/s]\u001b[A\n",
            "Iteration:  71% 258/361 [03:42<01:28,  1.17it/s]\u001b[A\n",
            "Iteration:  72% 259/361 [03:43<01:26,  1.18it/s]\u001b[A\n",
            "Iteration:  72% 260/361 [03:44<01:29,  1.13it/s]\u001b[A\n",
            "Iteration:  72% 261/361 [03:45<01:27,  1.15it/s]\u001b[A\n",
            "Iteration:  73% 262/361 [03:46<01:25,  1.16it/s]\u001b[A\n",
            "Iteration:  73% 263/361 [03:46<01:23,  1.17it/s]\u001b[A\n",
            "Iteration:  73% 264/361 [03:47<01:22,  1.17it/s]\u001b[A\n",
            "Iteration:  73% 265/361 [03:48<01:25,  1.12it/s]\u001b[A\n",
            "Iteration:  74% 266/361 [03:49<01:23,  1.14it/s]\u001b[A\n",
            "Iteration:  74% 267/361 [03:50<01:21,  1.15it/s]\u001b[A\n",
            "Iteration:  74% 268/361 [03:51<01:20,  1.16it/s]\u001b[A\n",
            "Iteration:  75% 269/361 [03:52<01:18,  1.17it/s]\u001b[A\n",
            "Iteration:  75% 270/361 [03:53<01:21,  1.12it/s]\u001b[A\n",
            "Iteration:  75% 271/361 [03:53<01:18,  1.14it/s]\u001b[A\n",
            "Iteration:  75% 272/361 [03:54<01:16,  1.16it/s]\u001b[A\n",
            "Iteration:  76% 273/361 [03:55<01:15,  1.17it/s]\u001b[A\n",
            "Iteration:  76% 274/361 [03:56<01:14,  1.17it/s]\u001b[A\n",
            "Iteration:  76% 275/361 [03:57<01:16,  1.12it/s]\u001b[A\n",
            "Iteration:  76% 276/361 [03:58<01:14,  1.14it/s]\u001b[A\n",
            "Iteration:  77% 277/361 [03:59<01:12,  1.16it/s]\u001b[A\n",
            "Iteration:  77% 278/361 [03:59<01:11,  1.16it/s]\u001b[A\n",
            "Iteration:  77% 279/361 [04:00<01:10,  1.17it/s]\u001b[A\n",
            "Iteration:  78% 280/361 [04:01<01:12,  1.12it/s]\u001b[A\n",
            "Iteration:  78% 281/361 [04:02<01:10,  1.14it/s]\u001b[A\n",
            "Iteration:  78% 282/361 [04:03<01:08,  1.15it/s]\u001b[A\n",
            "Iteration:  78% 283/361 [04:04<01:07,  1.16it/s]\u001b[A\n",
            "Iteration:  79% 284/361 [04:05<01:05,  1.17it/s]\u001b[A\n",
            "Iteration:  79% 285/361 [04:06<01:07,  1.12it/s]\u001b[A\n",
            "Iteration:  79% 286/361 [04:07<01:05,  1.14it/s]\u001b[A\n",
            "Iteration:  80% 287/361 [04:07<01:03,  1.16it/s]\u001b[A\n",
            "Iteration:  80% 288/361 [04:08<01:02,  1.17it/s]\u001b[A\n",
            "Iteration:  80% 289/361 [04:09<01:01,  1.17it/s]\u001b[A\n",
            "Iteration:  80% 290/361 [04:10<01:03,  1.12it/s]\u001b[A\n",
            "Iteration:  81% 291/361 [04:11<01:01,  1.14it/s]\u001b[A\n",
            "Iteration:  81% 292/361 [04:12<00:59,  1.16it/s]\u001b[A\n",
            "Iteration:  81% 293/361 [04:13<00:58,  1.16it/s]\u001b[A\n",
            "Iteration:  81% 294/361 [04:13<00:57,  1.17it/s]\u001b[A\n",
            "Iteration:  82% 295/361 [04:14<00:58,  1.12it/s]\u001b[A\n",
            "Iteration:  82% 296/361 [04:15<00:56,  1.14it/s]\u001b[A\n",
            "Iteration:  82% 297/361 [04:16<00:55,  1.16it/s]\u001b[A\n",
            "Iteration:  83% 298/361 [04:17<00:54,  1.16it/s]\u001b[A\n",
            "Iteration:  83% 299/361 [04:18<00:53,  1.17it/s]\u001b[A\n",
            "Iteration:  83% 300/361 [04:19<00:54,  1.12it/s]\u001b[A\n",
            "Iteration:  83% 301/361 [04:20<00:52,  1.14it/s]\u001b[A\n",
            "Iteration:  84% 302/361 [04:20<00:51,  1.15it/s]\u001b[A\n",
            "Iteration:  84% 303/361 [04:21<00:50,  1.16it/s]\u001b[A\n",
            "Iteration:  84% 304/361 [04:22<00:48,  1.17it/s]\u001b[A\n",
            "Iteration:  84% 305/361 [04:23<00:50,  1.12it/s]\u001b[A\n",
            "Iteration:  85% 306/361 [04:24<00:48,  1.14it/s]\u001b[A\n",
            "Iteration:  85% 307/361 [04:25<00:46,  1.15it/s]\u001b[A\n",
            "Iteration:  85% 308/361 [04:26<00:45,  1.16it/s]\u001b[A\n",
            "Iteration:  86% 309/361 [04:26<00:44,  1.16it/s]\u001b[A\n",
            "Iteration:  86% 310/361 [04:27<00:45,  1.12it/s]\u001b[A\n",
            "Iteration:  86% 311/361 [04:28<00:44,  1.14it/s]\u001b[A\n",
            "Iteration:  86% 312/361 [04:29<00:42,  1.15it/s]\u001b[A\n",
            "Iteration:  87% 313/361 [04:30<00:41,  1.16it/s]\u001b[A\n",
            "Iteration:  87% 314/361 [04:31<00:40,  1.16it/s]\u001b[A\n",
            "Iteration:  87% 315/361 [04:32<00:41,  1.12it/s]\u001b[A\n",
            "Iteration:  88% 316/361 [04:33<00:39,  1.14it/s]\u001b[A\n",
            "Iteration:  88% 317/361 [04:34<00:38,  1.15it/s]\u001b[A\n",
            "Iteration:  88% 318/361 [04:34<00:37,  1.16it/s]\u001b[A\n",
            "Iteration:  88% 319/361 [04:35<00:36,  1.16it/s]\u001b[A\n",
            "Iteration:  89% 320/361 [04:36<00:36,  1.12it/s]\u001b[A\n",
            "Iteration:  89% 321/361 [04:37<00:35,  1.14it/s]\u001b[A\n",
            "Iteration:  89% 322/361 [04:38<00:33,  1.16it/s]\u001b[A\n",
            "Iteration:  89% 323/361 [04:39<00:32,  1.16it/s]\u001b[A\n",
            "Iteration:  90% 324/361 [04:40<00:31,  1.17it/s]\u001b[A\n",
            "Iteration:  90% 325/361 [04:41<00:32,  1.12it/s]\u001b[A\n",
            "Iteration:  90% 326/361 [04:41<00:30,  1.13it/s]\u001b[A\n",
            "Iteration:  91% 327/361 [04:42<00:29,  1.15it/s]\u001b[A\n",
            "Iteration:  91% 328/361 [04:43<00:28,  1.16it/s]\u001b[A\n",
            "Iteration:  91% 329/361 [04:44<00:27,  1.16it/s]\u001b[A\n",
            "Iteration:  91% 330/361 [04:45<00:27,  1.12it/s]\u001b[A\n",
            "Iteration:  92% 331/361 [04:46<00:26,  1.14it/s]\u001b[A\n",
            "Iteration:  92% 332/361 [04:47<00:25,  1.15it/s]\u001b[A\n",
            "Iteration:  92% 333/361 [04:47<00:24,  1.16it/s]\u001b[A\n",
            "Iteration:  93% 334/361 [04:48<00:23,  1.16it/s]\u001b[A\n",
            "Iteration:  93% 335/361 [04:49<00:23,  1.11it/s]\u001b[A\n",
            "Iteration:  93% 336/361 [04:50<00:22,  1.14it/s]\u001b[A\n",
            "Iteration:  93% 337/361 [04:51<00:20,  1.15it/s]\u001b[A\n",
            "Iteration:  94% 338/361 [04:52<00:19,  1.16it/s]\u001b[A\n",
            "Iteration:  94% 339/361 [04:53<00:18,  1.16it/s]\u001b[A\n",
            "Iteration:  94% 340/361 [04:54<00:18,  1.12it/s]\u001b[A\n",
            "Iteration:  94% 341/361 [04:54<00:17,  1.14it/s]\u001b[A\n",
            "Iteration:  95% 342/361 [04:55<00:16,  1.15it/s]\u001b[A\n",
            "Iteration:  95% 343/361 [04:56<00:15,  1.16it/s]\u001b[A\n",
            "Iteration:  95% 344/361 [04:57<00:14,  1.17it/s]\u001b[A\n",
            "Iteration:  96% 345/361 [04:58<00:14,  1.12it/s]\u001b[A\n",
            "Iteration:  96% 346/361 [04:59<00:13,  1.14it/s]\u001b[A\n",
            "Iteration:  96% 347/361 [05:00<00:12,  1.15it/s]\u001b[A\n",
            "Iteration:  96% 348/361 [05:01<00:11,  1.16it/s]\u001b[A\n",
            "Iteration:  97% 349/361 [05:01<00:10,  1.17it/s]\u001b[A\n",
            "Iteration:  97% 350/361 [05:02<00:09,  1.11it/s]\u001b[A\n",
            "Iteration:  97% 351/361 [05:03<00:08,  1.14it/s]\u001b[A\n",
            "Iteration:  98% 352/361 [05:04<00:07,  1.15it/s]\u001b[A\n",
            "Iteration:  98% 353/361 [05:05<00:06,  1.16it/s]\u001b[A\n",
            "Iteration:  98% 354/361 [05:06<00:06,  1.16it/s]\u001b[A\n",
            "Iteration:  98% 355/361 [05:07<00:05,  1.11it/s]\u001b[A\n",
            "Iteration:  99% 356/361 [05:08<00:04,  1.14it/s]\u001b[A\n",
            "Iteration:  99% 357/361 [05:08<00:03,  1.16it/s]\u001b[A\n",
            "Iteration:  99% 358/361 [05:09<00:02,  1.16it/s]\u001b[A\n",
            "Iteration:  99% 359/361 [05:10<00:01,  1.17it/s]\u001b[A\n",
            "Iteration: 100% 360/361 [05:11<00:00,  1.12it/s]\u001b[A\n",
            "Iteration: 100% 361/361 [05:12<00:00,  1.16it/s]\n",
            "02/27/2021 21:15:36 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "02/27/2021 21:15:36 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_512_star_wars_valid.txt\n",
            "02/27/2021 21:15:36 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/27/2021 21:15:36 - INFO - __main__ -     Num examples = 83\n",
            "02/27/2021 21:15:36 - INFO - __main__ -     Batch size = 2\n",
            "\n",
            "Evaluating:   0% 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   2% 1/42 [00:00<00:10,  3.93it/s]\u001b[A\n",
            "Evaluating:   5% 2/42 [00:00<00:10,  3.84it/s]\u001b[A\n",
            "Evaluating:   7% 3/42 [00:00<00:10,  3.76it/s]\u001b[A\n",
            "Evaluating:  10% 4/42 [00:01<00:10,  3.56it/s]\u001b[A\n",
            "Evaluating:  12% 5/42 [00:01<00:10,  3.58it/s]\u001b[A\n",
            "Evaluating:  14% 6/42 [00:01<00:10,  3.59it/s]\u001b[A\n",
            "Evaluating:  17% 7/42 [00:01<00:10,  3.49it/s]\u001b[A\n",
            "Evaluating:  19% 8/42 [00:02<00:09,  3.49it/s]\u001b[A\n",
            "Evaluating:  21% 9/42 [00:02<00:09,  3.53it/s]\u001b[A\n",
            "Evaluating:  24% 10/42 [00:02<00:09,  3.51it/s]\u001b[A\n",
            "Evaluating:  26% 11/42 [00:03<00:08,  3.47it/s]\u001b[A\n",
            "Evaluating:  29% 12/42 [00:03<00:08,  3.50it/s]\u001b[A\n",
            "Evaluating:  31% 13/42 [00:03<00:08,  3.52it/s]\u001b[A\n",
            "Evaluating:  33% 14/42 [00:03<00:08,  3.47it/s]\u001b[A\n",
            "Evaluating:  36% 15/42 [00:04<00:07,  3.48it/s]\u001b[A\n",
            "Evaluating:  38% 16/42 [00:04<00:07,  3.52it/s]\u001b[A\n",
            "Evaluating:  40% 17/42 [00:04<00:07,  3.50it/s]\u001b[A\n",
            "Evaluating:  43% 18/42 [00:05<00:06,  3.47it/s]\u001b[A\n",
            "Evaluating:  45% 19/42 [00:05<00:06,  3.49it/s]\u001b[A\n",
            "Evaluating:  48% 20/42 [00:05<00:06,  3.50it/s]\u001b[A\n",
            "Evaluating:  50% 21/42 [00:05<00:06,  3.46it/s]\u001b[A\n",
            "Evaluating:  52% 22/42 [00:06<00:05,  3.46it/s]\u001b[A\n",
            "Evaluating:  55% 23/42 [00:06<00:05,  3.49it/s]\u001b[A\n",
            "Evaluating:  57% 24/42 [00:06<00:05,  3.48it/s]\u001b[A\n",
            "Evaluating:  60% 25/42 [00:07<00:04,  3.46it/s]\u001b[A\n",
            "Evaluating:  62% 26/42 [00:07<00:04,  3.47it/s]\u001b[A\n",
            "Evaluating:  64% 27/42 [00:07<00:04,  3.48it/s]\u001b[A\n",
            "Evaluating:  67% 28/42 [00:08<00:04,  3.47it/s]\u001b[A\n",
            "Evaluating:  69% 29/42 [00:08<00:03,  3.47it/s]\u001b[A\n",
            "Evaluating:  71% 30/42 [00:08<00:03,  3.49it/s]\u001b[A\n",
            "Evaluating:  74% 31/42 [00:08<00:03,  3.48it/s]\u001b[A\n",
            "Evaluating:  76% 32/42 [00:09<00:02,  3.47it/s]\u001b[A\n",
            "Evaluating:  79% 33/42 [00:09<00:02,  3.48it/s]\u001b[A\n",
            "Evaluating:  81% 34/42 [00:09<00:02,  3.49it/s]\u001b[A\n",
            "Evaluating:  83% 35/42 [00:10<00:02,  3.48it/s]\u001b[A\n",
            "Evaluating:  86% 36/42 [00:10<00:01,  3.46it/s]\u001b[A\n",
            "Evaluating:  88% 37/42 [00:10<00:01,  3.48it/s]\u001b[A\n",
            "Evaluating:  90% 38/42 [00:10<00:01,  3.47it/s]\u001b[A\n",
            "Evaluating:  93% 39/42 [00:11<00:00,  3.47it/s]\u001b[A\n",
            "Evaluating:  95% 40/42 [00:11<00:00,  3.48it/s]\u001b[A\n",
            "Evaluating:  98% 41/42 [00:11<00:00,  3.48it/s]\u001b[A\n",
            "Evaluating: 100% 42/42 [00:11<00:00,  3.53it/s]\n",
            "02/27/2021 21:15:48 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/27/2021 21:15:48 - INFO - __main__ -     perplexity = tensor(7.3822)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "02/27/2021 21:15:48 - INFO - __main__ -    global_step = 72, average loss = 2.1837222035974264\n",
            "Epoch:  33% 1/3 [05:24<10:49, 324.57s/it]\n",
            "Iteration:   0% 0/361 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/361 [00:00<04:45,  1.26it/s]\u001b[A\n",
            "Iteration:   1% 2/361 [00:01<04:50,  1.24it/s]\u001b[A\n",
            "Iteration:   1% 3/361 [00:02<04:53,  1.22it/s]\u001b[A\n",
            "Iteration:   1% 4/361 [00:03<04:54,  1.21it/s]\u001b[A\n",
            "Iteration:   1% 5/361 [00:04<05:09,  1.15it/s]\u001b[A\n",
            "Iteration:   2% 6/361 [00:05<05:05,  1.16it/s]\u001b[A\n",
            "Iteration:   2% 7/361 [00:05<05:02,  1.17it/s]\u001b[A\n",
            "Iteration:   2% 8/361 [00:06<04:59,  1.18it/s]\u001b[A\n",
            "Iteration:   2% 9/361 [00:07<04:57,  1.18it/s]\u001b[A\n",
            "Iteration:   3% 10/361 [00:08<05:10,  1.13it/s]\u001b[A\n",
            "Iteration:   3% 11/361 [00:09<05:03,  1.15it/s]\u001b[A\n",
            "Iteration:   3% 12/361 [00:10<04:58,  1.17it/s]\u001b[A\n",
            "Iteration:   4% 13/361 [00:11<04:55,  1.18it/s]\u001b[A\n",
            "Iteration:   4% 14/361 [00:11<04:53,  1.18it/s]\u001b[A\n",
            "Iteration:   4% 15/361 [00:12<05:06,  1.13it/s]\u001b[A\n",
            "Iteration:   4% 16/361 [00:13<04:59,  1.15it/s]\u001b[A\n",
            "Iteration:   5% 17/361 [00:14<04:55,  1.16it/s]\u001b[A\n",
            "Iteration:   5% 18/361 [00:15<04:51,  1.18it/s]\u001b[A\n",
            "Iteration:   5% 19/361 [00:16<04:48,  1.18it/s]\u001b[A\n",
            "Iteration:   6% 20/361 [00:17<05:00,  1.13it/s]\u001b[A\n",
            "Iteration:   6% 21/361 [00:18<04:55,  1.15it/s]\u001b[A\n",
            "Iteration:   6% 22/361 [00:18<04:50,  1.17it/s]\u001b[A\n",
            "Iteration:   6% 23/361 [00:19<04:47,  1.18it/s]\u001b[A\n",
            "Iteration:   7% 24/361 [00:20<04:44,  1.19it/s]\u001b[A\n",
            "Iteration:   7% 25/361 [00:21<04:56,  1.13it/s]\u001b[A\n",
            "Iteration:   7% 26/361 [00:22<04:50,  1.15it/s]\u001b[A\n",
            "Iteration:   7% 27/361 [00:23<04:45,  1.17it/s]\u001b[A\n",
            "Iteration:   8% 28/361 [00:24<04:42,  1.18it/s]\u001b[A\n",
            "Iteration:   8% 29/361 [00:24<04:40,  1.18it/s]\u001b[A\n",
            "Iteration:   8% 30/361 [00:25<04:53,  1.13it/s]\u001b[A\n",
            "Iteration:   9% 31/361 [00:26<04:47,  1.15it/s]\u001b[A\n",
            "Iteration:   9% 32/361 [00:27<04:43,  1.16it/s]\u001b[A\n",
            "Iteration:   9% 33/361 [00:28<04:40,  1.17it/s]\u001b[A\n",
            "Iteration:   9% 34/361 [00:29<04:38,  1.18it/s]\u001b[A\n",
            "Iteration:  10% 35/361 [00:30<04:49,  1.13it/s]\u001b[A\n",
            "Iteration:  10% 36/361 [00:31<04:43,  1.15it/s]\u001b[A\n",
            "Iteration:  10% 37/361 [00:31<04:38,  1.16it/s]\u001b[A\n",
            "Iteration:  11% 38/361 [00:32<04:35,  1.17it/s]\u001b[A\n",
            "Iteration:  11% 39/361 [00:33<04:32,  1.18it/s]\u001b[A\n",
            "Iteration:  11% 40/361 [00:34<04:43,  1.13it/s]\u001b[A\n",
            "Iteration:  11% 41/361 [00:35<04:38,  1.15it/s]\u001b[A\n",
            "Iteration:  12% 42/361 [00:36<04:35,  1.16it/s]\u001b[A\n",
            "Iteration:  12% 43/361 [00:36<04:31,  1.17it/s]\u001b[A\n",
            "Iteration:  12% 44/361 [00:37<04:29,  1.17it/s]\u001b[A\n",
            "Iteration:  12% 45/361 [00:38<04:42,  1.12it/s]\u001b[A\n",
            "Iteration:  13% 46/361 [00:39<04:38,  1.13it/s]\u001b[A\n",
            "Iteration:  13% 47/361 [00:40<04:33,  1.15it/s]\u001b[A\n",
            "Iteration:  13% 48/361 [00:41<04:30,  1.16it/s]\u001b[A\n",
            "Iteration:  14% 49/361 [00:42<04:28,  1.16it/s]\u001b[A\n",
            "Iteration:  14% 50/361 [00:43<04:40,  1.11it/s]\u001b[A\n",
            "Iteration:  14% 51/361 [00:44<04:33,  1.13it/s]\u001b[A\n",
            "Iteration:  14% 52/361 [00:44<04:29,  1.15it/s]\u001b[A\n",
            "Iteration:  15% 53/361 [00:45<04:26,  1.16it/s]\u001b[A\n",
            "Iteration:  15% 54/361 [00:46<04:24,  1.16it/s]\u001b[A\n",
            "Iteration:  15% 55/361 [00:47<04:34,  1.11it/s]\u001b[A\n",
            "Iteration:  16% 56/361 [00:48<04:30,  1.13it/s]\u001b[A\n",
            "Iteration:  16% 57/361 [00:49<04:25,  1.15it/s]\u001b[A\n",
            "Iteration:  16% 58/361 [00:50<04:23,  1.15it/s]\u001b[A\n",
            "Iteration:  16% 59/361 [00:51<04:20,  1.16it/s]\u001b[A\n",
            "Iteration:  17% 60/361 [00:51<04:30,  1.11it/s]\u001b[A\n",
            "Iteration:  17% 61/361 [00:52<04:26,  1.13it/s]\u001b[A\n",
            "Iteration:  17% 62/361 [00:53<04:21,  1.14it/s]\u001b[A\n",
            "Iteration:  17% 63/361 [00:54<04:18,  1.15it/s]\u001b[A\n",
            "Iteration:  18% 64/361 [00:55<04:16,  1.16it/s]\u001b[A\n",
            "Iteration:  18% 65/361 [00:56<04:26,  1.11it/s]\u001b[A\n",
            "Iteration:  18% 66/361 [00:57<04:21,  1.13it/s]\u001b[A\n",
            "Iteration:  19% 67/361 [00:58<04:17,  1.14it/s]\u001b[A\n",
            "Iteration:  19% 68/361 [00:58<04:14,  1.15it/s]\u001b[A\n",
            "Iteration:  19% 69/361 [00:59<04:11,  1.16it/s]\u001b[A\n",
            "Iteration:  19% 70/361 [01:00<04:21,  1.11it/s]\u001b[A\n",
            "Iteration:  20% 71/361 [01:01<04:17,  1.13it/s]\u001b[A\n",
            "Iteration:  20% 72/361 [01:02<04:13,  1.14it/s]\u001b[A\n",
            "Iteration:  20% 73/361 [01:03<04:09,  1.15it/s]\u001b[A\n",
            "Iteration:  20% 74/361 [01:04<04:07,  1.16it/s]\u001b[A\n",
            "Iteration:  21% 75/361 [01:05<04:17,  1.11it/s]\u001b[A\n",
            "Iteration:  21% 76/361 [01:06<04:12,  1.13it/s]\u001b[A\n",
            "Iteration:  21% 77/361 [01:06<04:08,  1.14it/s]\u001b[A\n",
            "Iteration:  22% 78/361 [01:07<04:05,  1.15it/s]\u001b[A\n",
            "Iteration:  22% 79/361 [01:08<04:03,  1.16it/s]\u001b[A\n",
            "Iteration:  22% 80/361 [01:09<04:13,  1.11it/s]\u001b[A\n",
            "Iteration:  22% 81/361 [01:10<04:08,  1.13it/s]\u001b[A\n",
            "Iteration:  23% 82/361 [01:11<04:04,  1.14it/s]\u001b[A\n",
            "Iteration:  23% 83/361 [01:12<04:01,  1.15it/s]\u001b[A\n",
            "Iteration:  23% 84/361 [01:12<03:59,  1.16it/s]\u001b[A\n",
            "Iteration:  24% 85/361 [01:13<04:08,  1.11it/s]\u001b[A\n",
            "Iteration:  24% 86/361 [01:14<04:04,  1.13it/s]\u001b[A\n",
            "Iteration:  24% 87/361 [01:15<03:59,  1.14it/s]\u001b[A\n",
            "Iteration:  24% 88/361 [01:16<03:56,  1.15it/s]\u001b[A\n",
            "Iteration:  25% 89/361 [01:17<03:54,  1.16it/s]\u001b[A\n",
            "Iteration:  25% 90/361 [01:18<04:04,  1.11it/s]\u001b[A\n",
            "Iteration:  25% 91/361 [01:19<03:59,  1.13it/s]\u001b[A\n",
            "Iteration:  25% 92/361 [01:20<03:54,  1.15it/s]\u001b[A\n",
            "Iteration:  26% 93/361 [01:20<03:51,  1.16it/s]\u001b[A\n",
            "Iteration:  26% 94/361 [01:21<03:49,  1.16it/s]\u001b[A\n",
            "Iteration:  26% 95/361 [01:22<03:59,  1.11it/s]\u001b[A\n",
            "Iteration:  27% 96/361 [01:23<03:55,  1.13it/s]\u001b[A\n",
            "Iteration:  27% 97/361 [01:24<03:49,  1.15it/s]\u001b[A\n",
            "Iteration:  27% 98/361 [01:25<03:46,  1.16it/s]\u001b[A\n",
            "Iteration:  27% 99/361 [01:26<03:44,  1.17it/s]\u001b[A\n",
            "Iteration:  28% 100/361 [01:27<03:52,  1.12it/s]\u001b[A\n",
            "Iteration:  28% 101/361 [01:27<03:47,  1.14it/s]\u001b[A\n",
            "Iteration:  28% 102/361 [01:28<03:44,  1.15it/s]\u001b[A\n",
            "Iteration:  29% 103/361 [01:29<03:42,  1.16it/s]\u001b[A\n",
            "Iteration:  29% 104/361 [01:30<03:39,  1.17it/s]\u001b[A\n",
            "Iteration:  29% 105/361 [01:31<03:49,  1.12it/s]\u001b[A\n",
            "Iteration:  29% 106/361 [01:32<03:45,  1.13it/s]\u001b[A\n",
            "Iteration:  30% 107/361 [01:33<03:39,  1.16it/s]\u001b[A\n",
            "Iteration:  30% 108/361 [01:33<03:36,  1.17it/s]\u001b[A\n",
            "Iteration:  30% 109/361 [01:34<03:34,  1.17it/s]\u001b[A\n",
            "Iteration:  30% 110/361 [01:35<03:42,  1.13it/s]\u001b[A\n",
            "Iteration:  31% 111/361 [01:36<03:38,  1.15it/s]\u001b[A\n",
            "Iteration:  31% 112/361 [01:37<03:34,  1.16it/s]\u001b[A\n",
            "Iteration:  31% 113/361 [01:38<03:32,  1.17it/s]\u001b[A\n",
            "Iteration:  32% 114/361 [01:39<03:29,  1.18it/s]\u001b[A\n",
            "Iteration:  32% 115/361 [01:40<03:37,  1.13it/s]\u001b[A\n",
            "Iteration:  32% 116/361 [01:40<03:32,  1.15it/s]\u001b[A\n",
            "Iteration:  32% 117/361 [01:41<03:29,  1.16it/s]\u001b[A\n",
            "Iteration:  33% 118/361 [01:42<03:26,  1.17it/s]\u001b[A\n",
            "Iteration:  33% 119/361 [01:43<03:24,  1.18it/s]\u001b[A\n",
            "Iteration:  33% 120/361 [01:44<03:33,  1.13it/s]\u001b[A\n",
            "Iteration:  34% 121/361 [01:45<03:28,  1.15it/s]\u001b[A\n",
            "Iteration:  34% 122/361 [01:46<03:25,  1.16it/s]\u001b[A\n",
            "Iteration:  34% 123/361 [01:46<03:22,  1.18it/s]\u001b[A\n",
            "Iteration:  34% 124/361 [01:47<03:20,  1.18it/s]\u001b[A\n",
            "Iteration:  35% 125/361 [01:48<03:30,  1.12it/s]\u001b[A\n",
            "Iteration:  35% 126/361 [01:49<03:24,  1.15it/s]\u001b[A\n",
            "Iteration:  35% 127/361 [01:50<03:20,  1.17it/s]\u001b[A\n",
            "Iteration:  35% 128/361 [01:51<03:18,  1.17it/s]\u001b[A\n",
            "Iteration:  36% 129/361 [01:52<03:16,  1.18it/s]\u001b[A\n",
            "Iteration:  36% 130/361 [01:53<03:25,  1.12it/s]\u001b[A\n",
            "Iteration:  36% 131/361 [01:53<03:22,  1.14it/s]\u001b[A\n",
            "Iteration:  37% 132/361 [01:54<03:17,  1.16it/s]\u001b[A\n",
            "Iteration:  37% 133/361 [01:55<03:14,  1.17it/s]\u001b[A\n",
            "Iteration:  37% 134/361 [01:56<03:12,  1.18it/s]\u001b[A\n",
            "Iteration:  37% 135/361 [01:57<03:20,  1.13it/s]\u001b[A\n",
            "Iteration:  38% 136/361 [01:58<03:16,  1.15it/s]\u001b[A\n",
            "Iteration:  38% 137/361 [01:59<03:12,  1.16it/s]\u001b[A\n",
            "Iteration:  38% 138/361 [01:59<03:10,  1.17it/s]\u001b[A\n",
            "Iteration:  39% 139/361 [02:00<03:08,  1.18it/s]\u001b[A\n",
            "Iteration:  39% 140/361 [02:01<03:16,  1.13it/s]\u001b[A\n",
            "Iteration:  39% 141/361 [02:02<03:12,  1.14it/s]\u001b[A\n",
            "Iteration:  39% 142/361 [02:03<03:08,  1.16it/s]\u001b[A\n",
            "Iteration:  40% 143/361 [02:04<03:06,  1.17it/s]\u001b[A\n",
            "Iteration:  40% 144/361 [02:05<03:05,  1.17it/s]\u001b[A\n",
            "Iteration:  40% 145/361 [02:06<03:13,  1.12it/s]\u001b[A\n",
            "Iteration:  40% 146/361 [02:06<03:09,  1.14it/s]\u001b[A\n",
            "Iteration:  41% 147/361 [02:07<03:05,  1.15it/s]\u001b[A\n",
            "Iteration:  41% 148/361 [02:08<03:03,  1.16it/s]\u001b[A\n",
            "Iteration:  41% 149/361 [02:09<03:01,  1.17it/s]\u001b[A\n",
            "Iteration:  42% 150/361 [02:10<03:07,  1.13it/s]\u001b[A\n",
            "Iteration:  42% 151/361 [02:11<03:03,  1.14it/s]\u001b[A\n",
            "Iteration:  42% 152/361 [02:12<03:00,  1.16it/s]\u001b[A\n",
            "Iteration:  42% 153/361 [02:12<02:58,  1.17it/s]\u001b[A\n",
            "Iteration:  43% 154/361 [02:13<02:56,  1.17it/s]\u001b[A\n",
            "Iteration:  43% 155/361 [02:14<03:03,  1.12it/s]\u001b[A\n",
            "Iteration:  43% 156/361 [02:15<02:59,  1.14it/s]\u001b[A\n",
            "Iteration:  43% 157/361 [02:16<02:56,  1.16it/s]\u001b[A\n",
            "Iteration:  44% 158/361 [02:17<02:54,  1.16it/s]\u001b[A\n",
            "Iteration:  44% 159/361 [02:18<02:52,  1.17it/s]\u001b[A\n",
            "Iteration:  44% 160/361 [02:19<02:59,  1.12it/s]\u001b[A\n",
            "Iteration:  45% 161/361 [02:19<02:55,  1.14it/s]\u001b[A\n",
            "Iteration:  45% 162/361 [02:20<02:52,  1.15it/s]\u001b[A\n",
            "Iteration:  45% 163/361 [02:21<02:49,  1.17it/s]\u001b[A\n",
            "Iteration:  45% 164/361 [02:22<02:48,  1.17it/s]\u001b[A\n",
            "Iteration:  46% 165/361 [02:23<02:54,  1.12it/s]\u001b[A\n",
            "Iteration:  46% 166/361 [02:24<02:51,  1.14it/s]\u001b[A\n",
            "Iteration:  46% 167/361 [02:25<02:48,  1.15it/s]\u001b[A\n",
            "Iteration:  47% 168/361 [02:25<02:45,  1.16it/s]\u001b[A\n",
            "Iteration:  47% 169/361 [02:26<02:44,  1.17it/s]\u001b[A\n",
            "Iteration:  47% 170/361 [02:27<02:50,  1.12it/s]\u001b[A\n",
            "Iteration:  47% 171/361 [02:28<02:46,  1.14it/s]\u001b[A\n",
            "Iteration:  48% 172/361 [02:29<02:43,  1.15it/s]\u001b[A\n",
            "Iteration:  48% 173/361 [02:30<02:41,  1.17it/s]\u001b[A\n",
            "Iteration:  48% 174/361 [02:31<02:39,  1.17it/s]\u001b[A\n",
            "Iteration:  48% 175/361 [02:32<02:46,  1.12it/s]\u001b[A\n",
            "Iteration:  49% 176/361 [02:32<02:42,  1.14it/s]\u001b[A\n",
            "Iteration:  49% 177/361 [02:33<02:39,  1.15it/s]\u001b[A\n",
            "Iteration:  49% 178/361 [02:34<02:37,  1.16it/s]\u001b[A\n",
            "Iteration:  50% 179/361 [02:35<02:36,  1.16it/s]\u001b[A\n",
            "Iteration:  50% 180/361 [02:36<02:42,  1.11it/s]\u001b[A\n",
            "Iteration:  50% 181/361 [02:37<02:38,  1.13it/s]\u001b[A\n",
            "Iteration:  50% 182/361 [02:38<02:35,  1.15it/s]\u001b[A\n",
            "Iteration:  51% 183/361 [02:39<02:33,  1.16it/s]\u001b[A\n",
            "Iteration:  51% 184/361 [02:39<02:32,  1.16it/s]\u001b[A\n",
            "Iteration:  51% 185/361 [02:40<02:37,  1.11it/s]\u001b[A\n",
            "Iteration:  52% 186/361 [02:41<02:34,  1.13it/s]\u001b[A\n",
            "Iteration:  52% 187/361 [02:42<02:31,  1.15it/s]\u001b[A\n",
            "Iteration:  52% 188/361 [02:43<02:29,  1.16it/s]\u001b[A\n",
            "Iteration:  52% 189/361 [02:44<02:27,  1.16it/s]\u001b[A\n",
            "Iteration:  53% 190/361 [02:45<02:32,  1.12it/s]\u001b[A\n",
            "Iteration:  53% 191/361 [02:46<02:28,  1.14it/s]\u001b[A\n",
            "Iteration:  53% 192/361 [02:46<02:26,  1.15it/s]\u001b[A\n",
            "Iteration:  53% 193/361 [02:47<02:24,  1.16it/s]\u001b[A\n",
            "Iteration:  54% 194/361 [02:48<02:23,  1.17it/s]\u001b[A\n",
            "Iteration:  54% 195/361 [02:49<02:28,  1.12it/s]\u001b[A\n",
            "Iteration:  54% 196/361 [02:50<02:24,  1.14it/s]\u001b[A\n",
            "Iteration:  55% 197/361 [02:51<02:22,  1.15it/s]\u001b[A\n",
            "Iteration:  55% 198/361 [02:52<02:20,  1.16it/s]\u001b[A\n",
            "Iteration:  55% 199/361 [02:52<02:18,  1.17it/s]\u001b[A\n",
            "Iteration:  55% 200/361 [02:53<02:24,  1.12it/s]\u001b[A\n",
            "Iteration:  56% 201/361 [02:54<02:20,  1.14it/s]\u001b[A\n",
            "Iteration:  56% 202/361 [02:55<02:17,  1.16it/s]\u001b[A\n",
            "Iteration:  56% 203/361 [02:56<02:15,  1.16it/s]\u001b[A\n",
            "Iteration:  57% 204/361 [02:57<02:14,  1.17it/s]\u001b[A\n",
            "Iteration:  57% 205/361 [02:58<02:18,  1.12it/s]\u001b[A\n",
            "Iteration:  57% 206/361 [02:59<02:15,  1.14it/s]\u001b[A\n",
            "Iteration:  57% 207/361 [02:59<02:13,  1.16it/s]\u001b[A\n",
            "Iteration:  58% 208/361 [03:00<02:11,  1.16it/s]\u001b[A\n",
            "Iteration:  58% 209/361 [03:01<02:10,  1.17it/s]\u001b[A\n",
            "Iteration:  58% 210/361 [03:02<02:15,  1.12it/s]\u001b[A\n",
            "Iteration:  58% 211/361 [03:03<02:12,  1.14it/s]\u001b[A\n",
            "Iteration:  59% 212/361 [03:04<02:09,  1.15it/s]\u001b[A\n",
            "Iteration:  59% 213/361 [03:05<02:07,  1.16it/s]\u001b[A\n",
            "Iteration:  59% 214/361 [03:06<02:06,  1.16it/s]\u001b[A\n",
            "Iteration:  60% 215/361 [03:07<02:11,  1.11it/s]\u001b[A\n",
            "Iteration:  60% 216/361 [03:07<02:07,  1.13it/s]\u001b[A\n",
            "Iteration:  60% 217/361 [03:08<02:05,  1.15it/s]\u001b[A\n",
            "Iteration:  60% 218/361 [03:09<02:03,  1.16it/s]\u001b[A\n",
            "Iteration:  61% 219/361 [03:10<02:01,  1.16it/s]\u001b[A\n",
            "Iteration:  61% 220/361 [03:11<02:06,  1.12it/s]\u001b[A\n",
            "Iteration:  61% 221/361 [03:12<02:03,  1.14it/s]\u001b[A\n",
            "Iteration:  61% 222/361 [03:13<02:00,  1.15it/s]\u001b[A\n",
            "Iteration:  62% 223/361 [03:13<01:59,  1.16it/s]\u001b[A\n",
            "Iteration:  62% 224/361 [03:14<01:57,  1.16it/s]\u001b[A\n",
            "Iteration:  62% 225/361 [03:15<02:02,  1.11it/s]\u001b[A\n",
            "Iteration:  63% 226/361 [03:16<01:59,  1.13it/s]\u001b[A\n",
            "Iteration:  63% 227/361 [03:17<01:56,  1.15it/s]\u001b[A\n",
            "Iteration:  63% 228/361 [03:18<01:54,  1.16it/s]\u001b[A\n",
            "Iteration:  63% 229/361 [03:19<01:53,  1.16it/s]\u001b[A\n",
            "Iteration:  64% 230/361 [03:20<01:57,  1.12it/s]\u001b[A\n",
            "Iteration:  64% 231/361 [03:20<01:54,  1.14it/s]\u001b[A\n",
            "Iteration:  64% 232/361 [03:21<01:51,  1.15it/s]\u001b[A\n",
            "Iteration:  65% 233/361 [03:22<01:50,  1.16it/s]\u001b[A\n",
            "Iteration:  65% 234/361 [03:23<01:48,  1.17it/s]\u001b[A\n",
            "Iteration:  65% 235/361 [03:24<01:52,  1.12it/s]\u001b[A\n",
            "Iteration:  65% 236/361 [03:25<01:50,  1.13it/s]\u001b[A\n",
            "Iteration:  66% 237/361 [03:26<01:47,  1.15it/s]\u001b[A\n",
            "Iteration:  66% 238/361 [03:27<01:46,  1.16it/s]\u001b[A\n",
            "Iteration:  66% 239/361 [03:27<01:44,  1.16it/s]\u001b[A\n",
            "Iteration:  66% 240/361 [03:28<01:48,  1.11it/s]\u001b[A\n",
            "Iteration:  67% 241/361 [03:29<01:45,  1.13it/s]\u001b[A\n",
            "Iteration:  67% 242/361 [03:30<01:43,  1.15it/s]\u001b[A\n",
            "Iteration:  67% 243/361 [03:31<01:41,  1.16it/s]\u001b[A\n",
            "Iteration:  68% 244/361 [03:32<01:40,  1.17it/s]\u001b[A\n",
            "Iteration:  68% 245/361 [03:33<01:43,  1.12it/s]\u001b[A\n",
            "Iteration:  68% 246/361 [03:34<01:41,  1.14it/s]\u001b[A\n",
            "Iteration:  68% 247/361 [03:34<01:38,  1.15it/s]\u001b[A\n",
            "Iteration:  69% 248/361 [03:35<01:37,  1.16it/s]\u001b[A\n",
            "Iteration:  69% 249/361 [03:36<01:35,  1.17it/s]\u001b[A\n",
            "Iteration:  69% 250/361 [03:37<01:39,  1.12it/s]\u001b[A\n",
            "Iteration:  70% 251/361 [03:38<01:36,  1.14it/s]\u001b[A\n",
            "Iteration:  70% 252/361 [03:39<01:34,  1.15it/s]\u001b[A\n",
            "Iteration:  70% 253/361 [03:40<01:33,  1.16it/s]\u001b[A\n",
            "Iteration:  70% 254/361 [03:40<01:31,  1.17it/s]\u001b[A\n",
            "Iteration:  71% 255/361 [03:41<01:34,  1.12it/s]\u001b[A\n",
            "Iteration:  71% 256/361 [03:42<01:32,  1.14it/s]\u001b[A\n",
            "Iteration:  71% 257/361 [03:43<01:30,  1.15it/s]\u001b[A\n",
            "Iteration:  71% 258/361 [03:44<01:28,  1.16it/s]\u001b[A\n",
            "Iteration:  72% 259/361 [03:45<01:27,  1.17it/s]\u001b[A\n",
            "Iteration:  72% 260/361 [03:46<01:30,  1.12it/s]\u001b[A\n",
            "Iteration:  72% 261/361 [03:47<01:28,  1.14it/s]\u001b[A\n",
            "Iteration:  73% 262/361 [03:47<01:25,  1.16it/s]\u001b[A\n",
            "Iteration:  73% 263/361 [03:48<01:24,  1.16it/s]\u001b[A\n",
            "Iteration:  73% 264/361 [03:49<01:22,  1.17it/s]\u001b[A\n",
            "Iteration:  73% 265/361 [03:50<01:25,  1.12it/s]\u001b[A\n",
            "Iteration:  74% 266/361 [03:51<01:23,  1.14it/s]\u001b[A\n",
            "Iteration:  74% 267/361 [03:52<01:21,  1.16it/s]\u001b[A\n",
            "Iteration:  74% 268/361 [03:53<01:20,  1.16it/s]\u001b[A\n",
            "Iteration:  75% 269/361 [03:54<01:18,  1.17it/s]\u001b[A\n",
            "Iteration:  75% 270/361 [03:55<01:21,  1.12it/s]\u001b[A\n",
            "Iteration:  75% 271/361 [03:55<01:19,  1.14it/s]\u001b[A\n",
            "Iteration:  75% 272/361 [03:56<01:17,  1.15it/s]\u001b[A\n",
            "Iteration:  76% 273/361 [03:57<01:15,  1.16it/s]\u001b[A\n",
            "Iteration:  76% 274/361 [03:58<01:14,  1.16it/s]\u001b[A\n",
            "Iteration:  76% 275/361 [03:59<01:16,  1.12it/s]\u001b[A\n",
            "Iteration:  76% 276/361 [04:00<01:14,  1.14it/s]\u001b[A\n",
            "Iteration:  77% 277/361 [04:01<01:12,  1.15it/s]\u001b[A\n",
            "Iteration:  77% 278/361 [04:01<01:11,  1.16it/s]\u001b[A\n",
            "Iteration:  77% 279/361 [04:02<01:10,  1.17it/s]\u001b[A\n",
            "Iteration:  78% 280/361 [04:03<01:12,  1.12it/s]\u001b[A\n",
            "Iteration:  78% 281/361 [04:04<01:10,  1.14it/s]\u001b[A\n",
            "Iteration:  78% 282/361 [04:05<01:08,  1.15it/s]\u001b[A\n",
            "Iteration:  78% 283/361 [04:06<01:07,  1.16it/s]\u001b[A\n",
            "Iteration:  79% 284/361 [04:07<01:05,  1.17it/s]\u001b[A\n",
            "Iteration:  79% 285/361 [04:08<01:07,  1.12it/s]\u001b[A\n",
            "Iteration:  79% 286/361 [04:08<01:05,  1.14it/s]\u001b[A\n",
            "Iteration:  80% 287/361 [04:09<01:03,  1.16it/s]\u001b[A\n",
            "Iteration:  80% 288/361 [04:10<01:02,  1.17it/s]\u001b[A\n",
            "Iteration:  80% 289/361 [04:11<01:01,  1.17it/s]\u001b[A\n",
            "Iteration:  80% 290/361 [04:12<01:03,  1.12it/s]\u001b[A\n",
            "Iteration:  81% 291/361 [04:13<01:01,  1.14it/s]\u001b[A\n",
            "Iteration:  81% 292/361 [04:14<00:59,  1.16it/s]\u001b[A\n",
            "Iteration:  81% 293/361 [04:14<00:58,  1.16it/s]\u001b[A\n",
            "Iteration:  81% 294/361 [04:15<00:57,  1.17it/s]\u001b[A\n",
            "Iteration:  82% 295/361 [04:16<00:58,  1.12it/s]\u001b[A\n",
            "Iteration:  82% 296/361 [04:17<00:57,  1.14it/s]\u001b[A\n",
            "Iteration:  82% 297/361 [04:18<00:55,  1.16it/s]\u001b[A\n",
            "Iteration:  83% 298/361 [04:19<00:54,  1.16it/s]\u001b[A\n",
            "Iteration:  83% 299/361 [04:20<00:52,  1.17it/s]\u001b[A\n",
            "Iteration:  83% 300/361 [04:21<00:54,  1.12it/s]\u001b[A\n",
            "Iteration:  83% 301/361 [04:21<00:52,  1.14it/s]\u001b[A\n",
            "Iteration:  84% 302/361 [04:22<00:51,  1.15it/s]\u001b[A\n",
            "Iteration:  84% 303/361 [04:23<00:49,  1.16it/s]\u001b[A\n",
            "Iteration:  84% 304/361 [04:24<00:48,  1.17it/s]\u001b[A\n",
            "Iteration:  84% 305/361 [04:25<00:49,  1.12it/s]\u001b[A\n",
            "Iteration:  85% 306/361 [04:26<00:48,  1.14it/s]\u001b[A\n",
            "Iteration:  85% 307/361 [04:27<00:46,  1.16it/s]\u001b[A\n",
            "Iteration:  85% 308/361 [04:27<00:45,  1.17it/s]\u001b[A\n",
            "Iteration:  86% 309/361 [04:28<00:44,  1.17it/s]\u001b[A\n",
            "Iteration:  86% 310/361 [04:29<00:45,  1.12it/s]\u001b[A\n",
            "Iteration:  86% 311/361 [04:30<00:43,  1.14it/s]\u001b[A\n",
            "Iteration:  86% 312/361 [04:31<00:42,  1.16it/s]\u001b[A\n",
            "Iteration:  87% 313/361 [04:32<00:40,  1.17it/s]\u001b[A\n",
            "Iteration:  87% 314/361 [04:33<00:39,  1.18it/s]\u001b[A\n",
            "Iteration:  87% 315/361 [04:34<00:40,  1.13it/s]\u001b[A\n",
            "Iteration:  88% 316/361 [04:34<00:39,  1.15it/s]\u001b[A\n",
            "Iteration:  88% 317/361 [04:35<00:37,  1.16it/s]\u001b[A\n",
            "Iteration:  88% 318/361 [04:36<00:36,  1.17it/s]\u001b[A\n",
            "Iteration:  88% 319/361 [04:37<00:35,  1.18it/s]\u001b[A\n",
            "Iteration:  89% 320/361 [04:38<00:36,  1.13it/s]\u001b[A\n",
            "Iteration:  89% 321/361 [04:39<00:34,  1.15it/s]\u001b[A\n",
            "Iteration:  89% 322/361 [04:40<00:33,  1.16it/s]\u001b[A\n",
            "Iteration:  89% 323/361 [04:40<00:32,  1.17it/s]\u001b[A\n",
            "Iteration:  90% 324/361 [04:41<00:31,  1.18it/s]\u001b[A\n",
            "Iteration:  90% 325/361 [04:42<00:32,  1.12it/s]\u001b[A\n",
            "Iteration:  90% 326/361 [04:43<00:30,  1.14it/s]\u001b[A\n",
            "Iteration:  91% 327/361 [04:44<00:29,  1.16it/s]\u001b[A\n",
            "Iteration:  91% 328/361 [04:45<00:28,  1.17it/s]\u001b[A\n",
            "Iteration:  91% 329/361 [04:46<00:27,  1.18it/s]\u001b[A\n",
            "Iteration:  91% 330/361 [04:47<00:27,  1.13it/s]\u001b[A\n",
            "Iteration:  92% 331/361 [04:47<00:26,  1.15it/s]\u001b[A\n",
            "Iteration:  92% 332/361 [04:48<00:24,  1.16it/s]\u001b[A\n",
            "Iteration:  92% 333/361 [04:49<00:23,  1.17it/s]\u001b[A\n",
            "Iteration:  93% 334/361 [04:50<00:22,  1.18it/s]\u001b[A\n",
            "Iteration:  93% 335/361 [04:51<00:23,  1.13it/s]\u001b[A\n",
            "Iteration:  93% 336/361 [04:52<00:21,  1.15it/s]\u001b[A\n",
            "Iteration:  93% 337/361 [04:53<00:20,  1.16it/s]\u001b[A\n",
            "Iteration:  94% 338/361 [04:53<00:19,  1.17it/s]\u001b[A\n",
            "Iteration:  94% 339/361 [04:54<00:18,  1.18it/s]\u001b[A\n",
            "Iteration:  94% 340/361 [04:55<00:18,  1.13it/s]\u001b[A\n",
            "Iteration:  94% 341/361 [04:56<00:17,  1.14it/s]\u001b[A\n",
            "Iteration:  95% 342/361 [04:57<00:16,  1.16it/s]\u001b[A\n",
            "Iteration:  95% 343/361 [04:58<00:15,  1.17it/s]\u001b[A\n",
            "Iteration:  95% 344/361 [04:59<00:14,  1.18it/s]\u001b[A\n",
            "Iteration:  96% 345/361 [05:00<00:14,  1.13it/s]\u001b[A\n",
            "Iteration:  96% 346/361 [05:00<00:13,  1.15it/s]\u001b[A\n",
            "Iteration:  96% 347/361 [05:01<00:12,  1.16it/s]\u001b[A\n",
            "Iteration:  96% 348/361 [05:02<00:11,  1.17it/s]\u001b[A\n",
            "Iteration:  97% 349/361 [05:03<00:10,  1.18it/s]\u001b[A\n",
            "Iteration:  97% 350/361 [05:04<00:09,  1.13it/s]\u001b[A\n",
            "Iteration:  97% 351/361 [05:05<00:08,  1.15it/s]\u001b[A\n",
            "Iteration:  98% 352/361 [05:06<00:07,  1.17it/s]\u001b[A\n",
            "Iteration:  98% 353/361 [05:06<00:06,  1.17it/s]\u001b[A\n",
            "Iteration:  98% 354/361 [05:07<00:05,  1.18it/s]\u001b[A\n",
            "Iteration:  98% 355/361 [05:08<00:05,  1.13it/s]\u001b[A\n",
            "Iteration:  99% 356/361 [05:09<00:04,  1.15it/s]\u001b[A\n",
            "Iteration:  99% 357/361 [05:10<00:03,  1.17it/s]\u001b[A\n",
            "Iteration:  99% 358/361 [05:11<00:02,  1.17it/s]\u001b[A\n",
            "Iteration:  99% 359/361 [05:12<00:01,  1.18it/s]\u001b[A\n",
            "Iteration: 100% 360/361 [05:12<00:00,  1.13it/s]\u001b[A\n",
            "Iteration: 100% 361/361 [05:13<00:00,  1.15it/s]\n",
            "02/27/2021 21:21:02 - INFO - __main__ -   Loading features from cached file /content/gpt2_cached_lm_512_star_wars_valid.txt\n",
            "02/27/2021 21:21:02 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/27/2021 21:21:02 - INFO - __main__ -     Num examples = 83\n",
            "02/27/2021 21:21:02 - INFO - __main__ -     Batch size = 2\n",
            "\n",
            "Evaluating:   0% 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   2% 1/42 [00:00<00:10,  3.74it/s]\u001b[A\n",
            "Evaluating:   5% 2/42 [00:00<00:10,  3.65it/s]\u001b[A\n",
            "Evaluating:   7% 3/42 [00:00<00:10,  3.61it/s]\u001b[A\n",
            "Evaluating:  10% 4/42 [00:01<00:10,  3.56it/s]\u001b[A\n",
            "Evaluating:  12% 5/42 [00:01<00:10,  3.55it/s]\u001b[A\n",
            "Evaluating:  14% 6/42 [00:01<00:10,  3.52it/s]\u001b[A\n",
            "Evaluating:  17% 7/42 [00:01<00:09,  3.51it/s]\u001b[A\n",
            "Evaluating:  19% 8/42 [00:02<00:09,  3.52it/s]\u001b[A\n",
            "Evaluating:  21% 9/42 [00:02<00:09,  3.50it/s]\u001b[A\n",
            "Evaluating:  24% 10/42 [00:02<00:09,  3.50it/s]\u001b[A\n",
            "Evaluating:  26% 11/42 [00:03<00:08,  3.50it/s]\u001b[A\n",
            "Evaluating:  29% 12/42 [00:03<00:08,  3.51it/s]\u001b[A\n",
            "Evaluating:  31% 13/42 [00:03<00:08,  3.51it/s]\u001b[A\n",
            "Evaluating:  33% 14/42 [00:03<00:08,  3.49it/s]\u001b[A\n",
            "Evaluating:  36% 15/42 [00:04<00:07,  3.51it/s]\u001b[A\n",
            "Evaluating:  38% 16/42 [00:04<00:07,  3.50it/s]\u001b[A\n",
            "Evaluating:  40% 17/42 [00:04<00:07,  3.50it/s]\u001b[A\n",
            "Evaluating:  43% 18/42 [00:05<00:06,  3.50it/s]\u001b[A\n",
            "Evaluating:  45% 19/42 [00:05<00:06,  3.50it/s]\u001b[A\n",
            "Evaluating:  48% 20/42 [00:05<00:06,  3.50it/s]\u001b[A\n",
            "Evaluating:  50% 21/42 [00:05<00:05,  3.51it/s]\u001b[A\n",
            "Evaluating:  52% 22/42 [00:06<00:05,  3.51it/s]\u001b[A\n",
            "Evaluating:  55% 23/42 [00:06<00:05,  3.50it/s]\u001b[A\n",
            "Evaluating:  57% 24/42 [00:06<00:05,  3.49it/s]\u001b[A\n",
            "Evaluating:  60% 25/42 [00:07<00:04,  3.50it/s]\u001b[A\n",
            "Evaluating:  62% 26/42 [00:07<00:04,  3.50it/s]\u001b[A\n",
            "Evaluating:  64% 27/42 [00:07<00:04,  3.49it/s]\u001b[A\n",
            "Evaluating:  67% 28/42 [00:07<00:03,  3.50it/s]\u001b[A\n",
            "Evaluating:  69% 29/42 [00:08<00:03,  3.50it/s]\u001b[A\n",
            "Evaluating:  71% 30/42 [00:08<00:03,  3.51it/s]\u001b[A\n",
            "Evaluating:  74% 31/42 [00:08<00:03,  3.50it/s]\u001b[A\n",
            "Evaluating:  76% 32/42 [00:09<00:02,  3.49it/s]\u001b[A\n",
            "Evaluating:  79% 33/42 [00:09<00:02,  3.50it/s]\u001b[A\n",
            "Evaluating:  81% 34/42 [00:09<00:02,  3.50it/s]\u001b[A\n",
            "Evaluating:  83% 35/42 [00:09<00:02,  3.49it/s]\u001b[A\n",
            "Evaluating:  86% 36/42 [00:10<00:01,  3.49it/s]\u001b[A\n",
            "Evaluating:  88% 37/42 [00:10<00:01,  3.50it/s]\u001b[A\n",
            "Evaluating:  90% 38/42 [00:10<00:01,  3.50it/s]\u001b[A\n",
            "Evaluating:  93% 39/42 [00:11<00:00,  3.49it/s]\u001b[A\n",
            "Evaluating:  95% 40/42 [00:11<00:00,  3.49it/s]\u001b[A\n",
            "Evaluating:  98% 41/42 [00:11<00:00,  3.49it/s]\u001b[A\n",
            "Evaluating: 100% 42/42 [00:11<00:00,  3.54it/s]\n",
            "02/27/2021 21:21:14 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/27/2021 21:21:14 - INFO - __main__ -     perplexity = tensor(7.1154)\n",
            "02/27/2021 21:21:14 - INFO - __main__ -    global_step = 144, average loss = 2.013775070818762\n",
            "Epoch:  67% 2/3 [10:50<05:24, 324.91s/it]\n",
            "Iteration:   0% 0/361 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/361 [00:00<04:54,  1.22it/s]\u001b[A\n",
            "Iteration:   1% 2/361 [00:01<04:56,  1.21it/s]\u001b[A\n",
            "Iteration:   1% 3/361 [00:02<04:56,  1.21it/s]\u001b[A\n",
            "Iteration:   1% 4/361 [00:03<04:58,  1.20it/s]\u001b[A\n",
            "Iteration:   1% 5/361 [00:04<05:12,  1.14it/s]\u001b[A\n",
            "Iteration:   2% 6/361 [00:05<05:07,  1.16it/s]\u001b[A\n",
            "Iteration:   2% 7/361 [00:05<05:03,  1.17it/s]\u001b[A\n",
            "Iteration:   2% 8/361 [00:06<05:00,  1.18it/s]\u001b[A\n",
            "Iteration:   2% 9/361 [00:07<04:59,  1.18it/s]\u001b[A\n",
            "Iteration:   3% 10/361 [00:08<05:12,  1.12it/s]\u001b[A\n",
            "Iteration:   3% 11/361 [00:09<05:06,  1.14it/s]\u001b[A\n",
            "Iteration:   3% 12/361 [00:10<05:00,  1.16it/s]\u001b[A\n",
            "Iteration:   4% 13/361 [00:11<04:57,  1.17it/s]\u001b[A\n",
            "Iteration:   4% 14/361 [00:12<04:55,  1.17it/s]\u001b[A\n",
            "Iteration:   4% 15/361 [00:13<05:08,  1.12it/s]\u001b[A\n",
            "Iteration:   4% 16/361 [00:13<05:01,  1.14it/s]\u001b[A\n",
            "Iteration:   5% 17/361 [00:14<04:57,  1.16it/s]\u001b[A\n",
            "Iteration:   5% 18/361 [00:15<04:53,  1.17it/s]\u001b[A\n",
            "Iteration:   5% 19/361 [00:16<04:51,  1.17it/s]\u001b[A\n",
            "Iteration:   6% 20/361 [00:17<05:04,  1.12it/s]\u001b[A\n",
            "Iteration:   6% 21/361 [00:18<04:57,  1.14it/s]\u001b[A\n",
            "Iteration:   6% 22/361 [00:19<04:52,  1.16it/s]\u001b[A\n",
            "Iteration:   6% 23/361 [00:19<04:49,  1.17it/s]\u001b[A\n",
            "Iteration:   7% 24/361 [00:20<04:47,  1.17it/s]\u001b[A\n",
            "Iteration:   7% 25/361 [00:21<04:57,  1.13it/s]\u001b[A\n",
            "Iteration:   7% 26/361 [00:22<04:52,  1.14it/s]\u001b[A\n",
            "Iteration:   7% 27/361 [00:23<04:47,  1.16it/s]\u001b[A\n",
            "Iteration:   8% 28/361 [00:24<04:45,  1.17it/s]\u001b[A\n",
            "Iteration:   8% 29/361 [00:25<04:41,  1.18it/s]\u001b[A\n",
            "Iteration:   8% 30/361 [00:25<04:53,  1.13it/s]\u001b[A\n",
            "Iteration:   9% 31/361 [00:26<04:47,  1.15it/s]\u001b[A\n",
            "Iteration:   9% 32/361 [00:27<04:43,  1.16it/s]\u001b[A\n",
            "Iteration:   9% 33/361 [00:28<04:41,  1.17it/s]\u001b[A\n",
            "Iteration:   9% 34/361 [00:29<04:38,  1.17it/s]\u001b[A\n",
            "Iteration:  10% 35/361 [00:30<04:50,  1.12it/s]\u001b[A\n",
            "Iteration:  10% 36/361 [00:31<04:46,  1.14it/s]\u001b[A\n",
            "Iteration:  10% 37/361 [00:32<04:41,  1.15it/s]\u001b[A\n",
            "Iteration:  11% 38/361 [00:32<04:37,  1.17it/s]\u001b[A\n",
            "Iteration:  11% 39/361 [00:33<04:34,  1.17it/s]\u001b[A\n",
            "Iteration:  11% 40/361 [00:34<04:45,  1.13it/s]\u001b[A\n",
            "Iteration:  11% 41/361 [00:35<04:39,  1.14it/s]\u001b[A\n",
            "Iteration:  12% 42/361 [00:36<04:35,  1.16it/s]\u001b[A\n",
            "Iteration:  12% 43/361 [00:37<04:32,  1.17it/s]\u001b[A\n",
            "Iteration:  12% 44/361 [00:38<04:30,  1.17it/s]\u001b[A\n",
            "Iteration:  12% 45/361 [00:39<04:41,  1.12it/s]\u001b[A\n",
            "Iteration:  13% 46/361 [00:39<04:36,  1.14it/s]\u001b[A\n",
            "Iteration:  13% 47/361 [00:40<04:31,  1.16it/s]\u001b[A\n",
            "Iteration:  13% 48/361 [00:41<04:28,  1.17it/s]\u001b[A\n",
            "Iteration:  14% 49/361 [00:42<04:26,  1.17it/s]\u001b[A\n",
            "Iteration:  14% 50/361 [00:43<04:36,  1.12it/s]\u001b[A\n",
            "Iteration:  14% 51/361 [00:44<04:31,  1.14it/s]\u001b[A\n",
            "Iteration:  14% 52/361 [00:45<04:26,  1.16it/s]\u001b[A\n",
            "Iteration:  15% 53/361 [00:45<04:23,  1.17it/s]\u001b[A\n",
            "Iteration:  15% 54/361 [00:46<04:21,  1.18it/s]\u001b[A\n",
            "Iteration:  15% 55/361 [00:47<04:32,  1.12it/s]\u001b[A\n",
            "Iteration:  16% 56/361 [00:48<04:27,  1.14it/s]\u001b[A\n",
            "Iteration:  16% 57/361 [00:49<04:23,  1.15it/s]\u001b[A\n",
            "Iteration:  16% 58/361 [00:50<04:19,  1.17it/s]\u001b[A\n",
            "Iteration:  16% 59/361 [00:51<04:17,  1.17it/s]\u001b[A\n",
            "Iteration:  17% 60/361 [00:52<04:28,  1.12it/s]\u001b[A\n",
            "Iteration:  17% 61/361 [00:52<04:23,  1.14it/s]\u001b[A\n",
            "Iteration:  17% 62/361 [00:53<04:19,  1.15it/s]\u001b[A\n",
            "Iteration:  17% 63/361 [00:54<04:15,  1.17it/s]\u001b[A\n",
            "Iteration:  18% 64/361 [00:55<04:13,  1.17it/s]\u001b[A\n",
            "Iteration:  18% 65/361 [00:56<04:25,  1.12it/s]\u001b[A\n",
            "Iteration:  18% 66/361 [00:57<04:19,  1.14it/s]\u001b[A\n",
            "Iteration:  19% 67/361 [00:58<04:14,  1.15it/s]\u001b[A\n",
            "Iteration:  19% 68/361 [00:58<04:12,  1.16it/s]\u001b[A\n",
            "Iteration:  19% 69/361 [00:59<04:09,  1.17it/s]\u001b[A\n",
            "Iteration:  19% 70/361 [01:00<04:19,  1.12it/s]\u001b[A\n",
            "Iteration:  20% 71/361 [01:01<04:13,  1.14it/s]\u001b[A\n",
            "Iteration:  20% 72/361 [01:02<04:09,  1.16it/s]\u001b[A\n",
            "Iteration:  20% 73/361 [01:03<04:07,  1.16it/s]\u001b[A\n",
            "Iteration:  20% 74/361 [01:04<04:05,  1.17it/s]\u001b[A\n",
            "Iteration:  21% 75/361 [01:05<04:14,  1.12it/s]\u001b[A\n",
            "Iteration:  21% 76/361 [01:05<04:09,  1.14it/s]\u001b[A\n",
            "Iteration:  21% 77/361 [01:06<04:04,  1.16it/s]\u001b[A\n",
            "Iteration:  22% 78/361 [01:07<04:02,  1.17it/s]\u001b[A\n",
            "Iteration:  22% 79/361 [01:08<04:00,  1.17it/s]\u001b[A\n",
            "Iteration:  22% 80/361 [01:09<04:11,  1.12it/s]\u001b[A\n",
            "Iteration:  22% 81/361 [01:10<04:06,  1.14it/s]\u001b[A\n",
            "Iteration:  23% 82/361 [01:11<04:01,  1.15it/s]\u001b[A\n",
            "Iteration:  23% 83/361 [01:11<03:59,  1.16it/s]\u001b[A\n",
            "Iteration:  23% 84/361 [01:12<03:56,  1.17it/s]\u001b[A\n",
            "Iteration:  24% 85/361 [01:13<04:06,  1.12it/s]\u001b[A\n",
            "Iteration:  24% 86/361 [01:14<04:01,  1.14it/s]\u001b[A\n",
            "Iteration:  24% 87/361 [01:15<03:57,  1.15it/s]\u001b[A\n",
            "Iteration:  24% 88/361 [01:16<03:55,  1.16it/s]\u001b[A\n",
            "Iteration:  25% 89/361 [01:17<03:53,  1.17it/s]\u001b[A\n",
            "Iteration:  25% 90/361 [01:18<04:02,  1.12it/s]\u001b[A\n",
            "Iteration:  25% 91/361 [01:19<03:57,  1.14it/s]\u001b[A\n",
            "Iteration:  25% 92/361 [01:19<03:52,  1.16it/s]\u001b[A\n",
            "Iteration:  26% 93/361 [01:20<03:50,  1.16it/s]\u001b[A\n",
            "Iteration:  26% 94/361 [01:21<03:47,  1.17it/s]\u001b[A\n",
            "Iteration:  26% 95/361 [01:22<03:58,  1.12it/s]\u001b[A\n",
            "Iteration:  27% 96/361 [01:23<03:52,  1.14it/s]\u001b[A\n",
            "Iteration:  27% 97/361 [01:24<03:48,  1.16it/s]\u001b[A\n",
            "Iteration:  27% 98/361 [01:25<03:45,  1.17it/s]\u001b[A\n",
            "Iteration:  27% 99/361 [01:25<03:43,  1.17it/s]\u001b[A\n",
            "Iteration:  28% 100/361 [01:26<03:53,  1.12it/s]\u001b[A\n",
            "Iteration:  28% 101/361 [01:27<03:48,  1.14it/s]\u001b[A\n",
            "Iteration:  28% 102/361 [01:28<03:43,  1.16it/s]\u001b[A\n",
            "Iteration:  29% 103/361 [01:29<03:41,  1.16it/s]\u001b[A\n",
            "Iteration:  29% 104/361 [01:30<03:39,  1.17it/s]\u001b[A\n",
            "Iteration:  29% 105/361 [01:31<03:49,  1.11it/s]\u001b[A\n",
            "Iteration:  29% 106/361 [01:32<03:44,  1.13it/s]\u001b[A\n",
            "Iteration:  30% 107/361 [01:32<03:41,  1.15it/s]\u001b[A\n",
            "Iteration:  30% 108/361 [01:33<03:38,  1.16it/s]\u001b[A\n",
            "Iteration:  30% 109/361 [01:34<03:35,  1.17it/s]\u001b[A\n",
            "Iteration:  30% 110/361 [01:35<03:43,  1.12it/s]\u001b[A\n",
            "Iteration:  31% 111/361 [01:36<03:38,  1.14it/s]\u001b[A\n",
            "Iteration:  31% 112/361 [01:37<03:35,  1.16it/s]\u001b[A\n",
            "Iteration:  31% 113/361 [01:38<03:32,  1.17it/s]\u001b[A\n",
            "Iteration:  32% 114/361 [01:38<03:30,  1.17it/s]\u001b[A\n",
            "Iteration:  32% 115/361 [01:39<03:40,  1.12it/s]\u001b[A\n",
            "Iteration:  32% 116/361 [01:40<03:35,  1.14it/s]\u001b[A\n",
            "Iteration:  32% 117/361 [01:41<03:31,  1.15it/s]\u001b[A\n",
            "Iteration:  33% 118/361 [01:42<03:29,  1.16it/s]\u001b[A\n",
            "Iteration:  33% 119/361 [01:43<03:26,  1.17it/s]\u001b[A\n",
            "Iteration:  33% 120/361 [01:44<03:34,  1.12it/s]\u001b[A\n",
            "Iteration:  34% 121/361 [01:45<03:30,  1.14it/s]\u001b[A\n",
            "Iteration:  34% 122/361 [01:45<03:27,  1.15it/s]\u001b[A\n",
            "Iteration:  34% 123/361 [01:46<03:25,  1.16it/s]\u001b[A\n",
            "Iteration:  34% 124/361 [01:47<03:23,  1.17it/s]\u001b[A\n",
            "Iteration:  35% 125/361 [01:48<03:31,  1.12it/s]\u001b[A\n",
            "Iteration:  35% 126/361 [01:49<03:26,  1.14it/s]\u001b[A\n",
            "Iteration:  35% 127/361 [01:50<03:22,  1.16it/s]\u001b[A\n",
            "Iteration:  35% 128/361 [01:51<03:20,  1.16it/s]\u001b[A\n",
            "Iteration:  36% 129/361 [01:51<03:18,  1.17it/s]\u001b[A\n",
            "Iteration:  36% 130/361 [01:52<03:25,  1.12it/s]\u001b[A\n",
            "Iteration:  36% 131/361 [01:53<03:21,  1.14it/s]\u001b[A\n",
            "Iteration:  37% 132/361 [01:54<03:17,  1.16it/s]\u001b[A\n",
            "Iteration:  37% 133/361 [01:55<03:15,  1.17it/s]\u001b[A\n",
            "Iteration:  37% 134/361 [01:56<03:13,  1.18it/s]\u001b[A\n",
            "Iteration:  37% 135/361 [01:57<03:21,  1.12it/s]\u001b[A\n",
            "Iteration:  38% 136/361 [01:58<03:17,  1.14it/s]\u001b[A\n",
            "Iteration:  38% 137/361 [01:58<03:13,  1.16it/s]\u001b[A\n",
            "Iteration:  38% 138/361 [01:59<03:11,  1.17it/s]\u001b[A\n",
            "Iteration:  39% 139/361 [02:00<03:09,  1.17it/s]\u001b[A\n",
            "Iteration:  39% 140/361 [02:01<03:17,  1.12it/s]\u001b[A\n",
            "Iteration:  39% 141/361 [02:02<03:13,  1.14it/s]\u001b[A\n",
            "Iteration:  39% 142/361 [02:03<03:09,  1.15it/s]\u001b[A\n",
            "Iteration:  40% 143/361 [02:04<03:07,  1.16it/s]\u001b[A\n",
            "Iteration:  40% 144/361 [02:05<03:05,  1.17it/s]\u001b[A\n",
            "Iteration:  40% 145/361 [02:06<03:13,  1.12it/s]\u001b[A\n",
            "Iteration:  40% 146/361 [02:06<03:08,  1.14it/s]\u001b[A\n",
            "Iteration:  41% 147/361 [02:07<03:04,  1.16it/s]\u001b[A\n",
            "Iteration:  41% 148/361 [02:08<03:02,  1.17it/s]\u001b[A\n",
            "Iteration:  41% 149/361 [02:09<03:01,  1.17it/s]\u001b[A\n",
            "Iteration:  42% 150/361 [02:10<03:08,  1.12it/s]\u001b[A\n",
            "Iteration:  42% 151/361 [02:11<03:03,  1.14it/s]\u001b[A\n",
            "Iteration:  42% 152/361 [02:12<03:00,  1.16it/s]\u001b[A\n",
            "Iteration:  42% 153/361 [02:12<02:58,  1.17it/s]\u001b[A\n",
            "Iteration:  43% 154/361 [02:13<02:56,  1.17it/s]\u001b[A\n",
            "Iteration:  43% 155/361 [02:14<03:03,  1.12it/s]\u001b[A\n",
            "Iteration:  43% 156/361 [02:15<03:00,  1.14it/s]\u001b[A\n",
            "Iteration:  43% 157/361 [02:16<02:56,  1.15it/s]\u001b[A\n",
            "Iteration:  44% 158/361 [02:17<02:54,  1.17it/s]\u001b[A\n",
            "Iteration:  44% 159/361 [02:18<02:52,  1.17it/s]\u001b[A\n",
            "Iteration:  44% 160/361 [02:19<02:59,  1.12it/s]\u001b[A\n",
            "Iteration:  45% 161/361 [02:19<02:55,  1.14it/s]\u001b[A\n",
            "Iteration:  45% 162/361 [02:20<02:52,  1.15it/s]\u001b[A\n",
            "Iteration:  45% 163/361 [02:21<02:50,  1.16it/s]\u001b[A\n",
            "Iteration:  45% 164/361 [02:22<02:48,  1.17it/s]\u001b[A\n",
            "Iteration:  46% 165/361 [02:23<02:54,  1.12it/s]\u001b[A\n",
            "Iteration:  46% 166/361 [02:24<02:50,  1.14it/s]\u001b[A\n",
            "Iteration:  46% 167/361 [02:25<02:47,  1.16it/s]\u001b[A\n",
            "Iteration:  47% 168/361 [02:25<02:45,  1.17it/s]\u001b[A\n",
            "Iteration:  47% 169/361 [02:26<02:44,  1.17it/s]\u001b[A\n",
            "Iteration:  47% 170/361 [02:27<02:50,  1.12it/s]\u001b[A\n",
            "Iteration:  47% 171/361 [02:28<02:46,  1.14it/s]\u001b[A\n",
            "Iteration:  48% 172/361 [02:29<02:43,  1.16it/s]\u001b[A\n",
            "Iteration:  48% 173/361 [02:30<02:41,  1.16it/s]\u001b[A\n",
            "Iteration:  48% 174/361 [02:31<02:39,  1.17it/s]\u001b[A\n",
            "Iteration:  48% 175/361 [02:32<02:45,  1.12it/s]\u001b[A\n",
            "Iteration:  49% 176/361 [02:32<02:42,  1.14it/s]\u001b[A\n",
            "Iteration:  49% 177/361 [02:33<02:38,  1.16it/s]\u001b[A\n",
            "Iteration:  49% 178/361 [02:34<02:37,  1.16it/s]\u001b[A\n",
            "Iteration:  50% 179/361 [02:35<02:35,  1.17it/s]\u001b[A\n",
            "Iteration:  50% 180/361 [02:36<02:41,  1.12it/s]\u001b[A\n",
            "Iteration:  50% 181/361 [02:37<02:37,  1.14it/s]\u001b[A\n",
            "Iteration:  50% 182/361 [02:38<02:34,  1.16it/s]\u001b[A\n",
            "Iteration:  51% 183/361 [02:38<02:32,  1.17it/s]\u001b[A\n",
            "Iteration:  51% 184/361 [02:39<02:31,  1.17it/s]\u001b[A\n",
            "Iteration:  51% 185/361 [02:40<02:36,  1.12it/s]\u001b[A\n",
            "Iteration:  52% 186/361 [02:41<02:33,  1.14it/s]\u001b[A\n",
            "Iteration:  52% 187/361 [02:42<02:30,  1.15it/s]\u001b[A\n",
            "Iteration:  52% 188/361 [02:43<02:28,  1.16it/s]\u001b[A\n",
            "Iteration:  52% 189/361 [02:44<02:26,  1.17it/s]\u001b[A\n",
            "Iteration:  53% 190/361 [02:45<02:32,  1.12it/s]\u001b[A\n",
            "Iteration:  53% 191/361 [02:45<02:29,  1.14it/s]\u001b[A\n",
            "Iteration:  53% 192/361 [02:46<02:26,  1.15it/s]\u001b[A\n",
            "Iteration:  53% 193/361 [02:47<02:24,  1.16it/s]\u001b[A\n",
            "Iteration:  54% 194/361 [02:48<02:22,  1.17it/s]\u001b[A\n",
            "Iteration:  54% 195/361 [02:49<02:27,  1.12it/s]\u001b[A\n",
            "Iteration:  54% 196/361 [02:50<02:24,  1.14it/s]\u001b[A\n",
            "Iteration:  55% 197/361 [02:51<02:21,  1.16it/s]\u001b[A\n",
            "Iteration:  55% 198/361 [02:51<02:19,  1.17it/s]\u001b[A\n",
            "Iteration:  55% 199/361 [02:52<02:18,  1.17it/s]\u001b[A\n",
            "Iteration:  55% 200/361 [02:53<02:23,  1.13it/s]\u001b[A\n",
            "Iteration:  56% 201/361 [02:54<02:19,  1.15it/s]\u001b[A\n",
            "Iteration:  56% 202/361 [02:55<02:17,  1.16it/s]\u001b[A\n",
            "Iteration:  56% 203/361 [02:56<02:15,  1.17it/s]\u001b[A\n",
            "Iteration:  57% 204/361 [02:57<02:13,  1.17it/s]\u001b[A\n",
            "Iteration:  57% 205/361 [02:58<02:18,  1.13it/s]\u001b[A\n",
            "Iteration:  57% 206/361 [02:58<02:15,  1.15it/s]\u001b[A\n",
            "Iteration:  57% 207/361 [02:59<02:13,  1.16it/s]\u001b[A\n",
            "Iteration:  58% 208/361 [03:00<02:11,  1.16it/s]\u001b[A\n",
            "Iteration:  58% 209/361 [03:01<02:10,  1.17it/s]\u001b[A\n",
            "Iteration:  58% 210/361 [03:02<02:14,  1.12it/s]\u001b[A\n",
            "Iteration:  58% 211/361 [03:03<02:11,  1.14it/s]\u001b[A\n",
            "Iteration:  59% 212/361 [03:04<02:08,  1.16it/s]\u001b[A\n",
            "Iteration:  59% 213/361 [03:05<02:07,  1.16it/s]\u001b[A\n",
            "Iteration:  59% 214/361 [03:05<02:05,  1.17it/s]\u001b[A\n",
            "Iteration:  60% 215/361 [03:06<02:10,  1.12it/s]\u001b[A\n",
            "Iteration:  60% 216/361 [03:07<02:07,  1.14it/s]\u001b[A\n",
            "Iteration:  60% 217/361 [03:08<02:04,  1.16it/s]\u001b[A\n",
            "Iteration:  60% 218/361 [03:09<02:02,  1.17it/s]\u001b[A\n",
            "Iteration:  61% 219/361 [03:10<02:01,  1.17it/s]\u001b[A\n",
            "Iteration:  61% 220/361 [03:11<02:05,  1.12it/s]\u001b[A\n",
            "Iteration:  61% 221/361 [03:12<02:03,  1.14it/s]\u001b[A\n",
            "Iteration:  61% 222/361 [03:12<02:00,  1.16it/s]\u001b[A\n",
            "Iteration:  62% 223/361 [03:13<01:58,  1.17it/s]\u001b[A\n",
            "Iteration:  62% 224/361 [03:14<01:57,  1.17it/s]\u001b[A\n",
            "Iteration:  62% 225/361 [03:15<02:00,  1.12it/s]\u001b[A\n",
            "Iteration:  63% 226/361 [03:16<01:58,  1.14it/s]\u001b[A\n",
            "Iteration:  63% 227/361 [03:17<01:55,  1.16it/s]\u001b[A\n",
            "Iteration:  63% 228/361 [03:18<01:53,  1.17it/s]\u001b[A\n",
            "Iteration:  63% 229/361 [03:18<01:52,  1.17it/s]\u001b[A\n",
            "Iteration:  64% 230/361 [03:19<01:56,  1.12it/s]\u001b[A\n",
            "Iteration:  64% 231/361 [03:20<01:53,  1.14it/s]\u001b[A\n",
            "Iteration:  64% 232/361 [03:21<01:51,  1.16it/s]\u001b[A\n",
            "Iteration:  65% 233/361 [03:22<01:50,  1.16it/s]\u001b[A\n",
            "Iteration:  65% 234/361 [03:23<01:48,  1.17it/s]\u001b[A\n",
            "Iteration:  65% 235/361 [03:24<01:52,  1.12it/s]\u001b[A\n",
            "Iteration:  65% 236/361 [03:25<01:49,  1.14it/s]\u001b[A\n",
            "Iteration:  66% 237/361 [03:25<01:47,  1.16it/s]\u001b[A\n",
            "Iteration:  66% 238/361 [03:26<01:45,  1.17it/s]\u001b[A\n",
            "Iteration:  66% 239/361 [03:27<01:43,  1.17it/s]\u001b[A\n",
            "Iteration:  66% 240/361 [03:28<01:47,  1.13it/s]\u001b[A\n",
            "Iteration:  67% 241/361 [03:29<01:44,  1.15it/s]\u001b[A\n",
            "Iteration:  67% 242/361 [03:30<01:42,  1.16it/s]\u001b[A\n",
            "Iteration:  67% 243/361 [03:31<01:41,  1.16it/s]\u001b[A\n",
            "Iteration:  68% 244/361 [03:31<01:39,  1.17it/s]\u001b[A\n",
            "Iteration:  68% 245/361 [03:32<01:42,  1.13it/s]\u001b[A\n",
            "Iteration:  68% 246/361 [03:33<01:40,  1.15it/s]\u001b[A\n",
            "Iteration:  68% 247/361 [03:34<01:38,  1.16it/s]\u001b[A\n",
            "Iteration:  69% 248/361 [03:35<01:36,  1.17it/s]\u001b[A\n",
            "Iteration:  69% 249/361 [03:36<01:35,  1.17it/s]\u001b[A\n",
            "Iteration:  69% 250/361 [03:37<01:39,  1.12it/s]\u001b[A\n",
            "Iteration:  70% 251/361 [03:38<01:36,  1.14it/s]\u001b[A\n",
            "Iteration:  70% 252/361 [03:38<01:34,  1.16it/s]\u001b[A\n",
            "Iteration:  70% 253/361 [03:39<01:32,  1.17it/s]\u001b[A\n",
            "Iteration:  70% 254/361 [03:40<01:31,  1.17it/s]\u001b[A\n",
            "Iteration:  71% 255/361 [03:41<01:34,  1.12it/s]\u001b[A\n",
            "Iteration:  71% 256/361 [03:42<01:31,  1.14it/s]\u001b[A\n",
            "Iteration:  71% 257/361 [03:43<01:29,  1.16it/s]\u001b[A\n",
            "Iteration:  71% 258/361 [03:44<01:28,  1.17it/s]\u001b[A\n",
            "Iteration:  72% 259/361 [03:44<01:27,  1.17it/s]\u001b[A\n",
            "Iteration:  72% 260/361 [03:45<01:30,  1.12it/s]\u001b[A\n",
            "Iteration:  72% 261/361 [03:46<01:27,  1.14it/s]\u001b[A\n",
            "Iteration:  73% 262/361 [03:47<01:25,  1.16it/s]\u001b[A\n",
            "Iteration:  73% 263/361 [03:48<01:24,  1.17it/s]\u001b[A\n",
            "Iteration:  73% 264/361 [03:49<01:22,  1.17it/s]\u001b[A\n",
            "Iteration:  73% 265/361 [03:50<01:25,  1.12it/s]\u001b[A\n",
            "Iteration:  74% 266/361 [03:51<01:23,  1.14it/s]\u001b[A\n",
            "Iteration:  74% 267/361 [03:51<01:20,  1.16it/s]\u001b[A\n",
            "Iteration:  74% 268/361 [03:52<01:19,  1.17it/s]\u001b[A\n",
            "Iteration:  75% 269/361 [03:53<01:18,  1.17it/s]\u001b[A\n",
            "Iteration:  75% 270/361 [03:54<01:20,  1.12it/s]\u001b[A\n",
            "Iteration:  75% 271/361 [03:55<01:18,  1.14it/s]\u001b[A\n",
            "Iteration:  75% 272/361 [03:56<01:16,  1.16it/s]\u001b[A\n",
            "Iteration:  76% 273/361 [03:57<01:15,  1.17it/s]\u001b[A\n",
            "Iteration:  76% 274/361 [03:57<01:14,  1.17it/s]\u001b[A\n",
            "Iteration:  76% 275/361 [03:58<01:16,  1.12it/s]\u001b[A\n",
            "Iteration:  76% 276/361 [03:59<01:14,  1.14it/s]\u001b[A\n",
            "Iteration:  77% 277/361 [04:00<01:12,  1.16it/s]\u001b[A\n",
            "Iteration:  77% 278/361 [04:01<01:11,  1.17it/s]\u001b[A\n",
            "Iteration:  77% 279/361 [04:02<01:09,  1.17it/s]\u001b[A\n",
            "Iteration:  78% 280/361 [04:03<01:11,  1.13it/s]\u001b[A\n",
            "Iteration:  78% 281/361 [04:04<01:09,  1.15it/s]\u001b[A\n",
            "Iteration:  78% 282/361 [04:04<01:08,  1.16it/s]\u001b[A\n",
            "Iteration:  78% 283/361 [04:05<01:07,  1.16it/s]\u001b[A\n",
            "Iteration:  79% 284/361 [04:06<01:05,  1.17it/s]\u001b[A\n",
            "Iteration:  79% 285/361 [04:07<01:07,  1.12it/s]\u001b[A\n",
            "Iteration:  79% 286/361 [04:08<01:05,  1.14it/s]\u001b[A\n",
            "Iteration:  80% 287/361 [04:09<01:04,  1.16it/s]\u001b[A\n",
            "Iteration:  80% 288/361 [04:10<01:02,  1.16it/s]\u001b[A\n",
            "Iteration:  80% 289/361 [04:10<01:01,  1.17it/s]\u001b[A\n",
            "Iteration:  80% 290/361 [04:11<01:02,  1.13it/s]\u001b[A\n",
            "Iteration:  81% 291/361 [04:12<01:01,  1.15it/s]\u001b[A\n",
            "Iteration:  81% 292/361 [04:13<00:59,  1.16it/s]\u001b[A\n",
            "Iteration:  81% 293/361 [04:14<00:58,  1.17it/s]\u001b[A\n",
            "Iteration:  81% 294/361 [04:15<00:57,  1.17it/s]\u001b[A\n",
            "Iteration:  82% 295/361 [04:16<00:58,  1.13it/s]\u001b[A\n",
            "Iteration:  82% 296/361 [04:17<00:56,  1.15it/s]\u001b[A\n",
            "Iteration:  82% 297/361 [04:17<00:55,  1.16it/s]\u001b[A\n",
            "Iteration:  83% 298/361 [04:18<00:53,  1.17it/s]\u001b[A\n",
            "Iteration:  83% 299/361 [04:19<00:52,  1.18it/s]\u001b[A\n",
            "Iteration:  83% 300/361 [04:20<00:54,  1.13it/s]\u001b[A\n",
            "Iteration:  83% 301/361 [04:21<00:52,  1.15it/s]\u001b[A\n",
            "Iteration:  84% 302/361 [04:22<00:50,  1.17it/s]\u001b[A\n",
            "Iteration:  84% 303/361 [04:23<00:49,  1.17it/s]\u001b[A\n",
            "Iteration:  84% 304/361 [04:23<00:48,  1.18it/s]\u001b[A\n",
            "Iteration:  84% 305/361 [04:24<00:49,  1.13it/s]\u001b[A\n",
            "Iteration:  85% 306/361 [04:25<00:47,  1.15it/s]\u001b[A\n",
            "Iteration:  85% 307/361 [04:26<00:46,  1.16it/s]\u001b[A\n",
            "Iteration:  85% 308/361 [04:27<00:45,  1.16it/s]\u001b[A\n",
            "Iteration:  86% 309/361 [04:28<00:44,  1.17it/s]\u001b[A\n",
            "Iteration:  86% 310/361 [04:29<00:45,  1.12it/s]\u001b[A\n",
            "Iteration:  86% 311/361 [04:30<00:43,  1.14it/s]\u001b[A\n",
            "Iteration:  86% 312/361 [04:30<00:42,  1.16it/s]\u001b[A\n",
            "Iteration:  87% 313/361 [04:31<00:41,  1.17it/s]\u001b[A\n",
            "Iteration:  87% 314/361 [04:32<00:39,  1.18it/s]\u001b[A\n",
            "Iteration:  87% 315/361 [04:33<00:40,  1.12it/s]\u001b[A\n",
            "Iteration:  88% 316/361 [04:34<00:39,  1.14it/s]\u001b[A\n",
            "Iteration:  88% 317/361 [04:35<00:37,  1.16it/s]\u001b[A\n",
            "Iteration:  88% 318/361 [04:36<00:36,  1.17it/s]\u001b[A\n",
            "Iteration:  88% 319/361 [04:36<00:35,  1.18it/s]\u001b[A\n",
            "Iteration:  89% 320/361 [04:37<00:36,  1.12it/s]\u001b[A\n",
            "Iteration:  89% 321/361 [04:38<00:35,  1.14it/s]\u001b[A\n",
            "Iteration:  89% 322/361 [04:39<00:33,  1.16it/s]\u001b[A\n",
            "Iteration:  89% 323/361 [04:40<00:32,  1.17it/s]\u001b[A\n",
            "Iteration:  90% 324/361 [04:41<00:31,  1.17it/s]\u001b[A\n",
            "Iteration:  90% 325/361 [04:42<00:32,  1.12it/s]\u001b[A\n",
            "Iteration:  90% 326/361 [04:43<00:30,  1.14it/s]\u001b[A\n",
            "Iteration:  91% 327/361 [04:43<00:29,  1.16it/s]\u001b[A\n",
            "Iteration:  91% 328/361 [04:44<00:28,  1.17it/s]\u001b[A\n",
            "Iteration:  91% 329/361 [04:45<00:27,  1.17it/s]\u001b[A\n",
            "Iteration:  91% 330/361 [04:46<00:27,  1.12it/s]\u001b[A\n",
            "Iteration:  92% 331/361 [04:47<00:26,  1.14it/s]\u001b[A\n",
            "Iteration:  92% 332/361 [04:48<00:24,  1.16it/s]\u001b[A\n",
            "Iteration:  92% 333/361 [04:49<00:23,  1.17it/s]\u001b[A\n",
            "Iteration:  93% 334/361 [04:49<00:22,  1.18it/s]\u001b[A\n",
            "Iteration:  93% 335/361 [04:50<00:23,  1.13it/s]\u001b[A\n",
            "Iteration:  93% 336/361 [04:51<00:21,  1.15it/s]\u001b[A\n",
            "Iteration:  93% 337/361 [04:52<00:20,  1.16it/s]\u001b[A\n",
            "Iteration:  94% 338/361 [04:53<00:19,  1.17it/s]\u001b[A\n",
            "Iteration:  94% 339/361 [04:54<00:18,  1.17it/s]\u001b[A\n",
            "Iteration:  94% 340/361 [04:55<00:18,  1.13it/s]\u001b[A\n",
            "Iteration:  94% 341/361 [04:56<00:17,  1.14it/s]\u001b[A\n",
            "Iteration:  95% 342/361 [04:56<00:16,  1.16it/s]\u001b[A\n",
            "Iteration:  95% 343/361 [04:57<00:15,  1.17it/s]\u001b[A\n",
            "Iteration:  95% 344/361 [04:58<00:14,  1.18it/s]\u001b[A\n",
            "Iteration:  96% 345/361 [04:59<00:14,  1.13it/s]\u001b[A\n",
            "Iteration:  96% 346/361 [05:00<00:13,  1.15it/s]\u001b[A\n",
            "Iteration:  96% 347/361 [05:01<00:12,  1.16it/s]\u001b[A\n",
            "Iteration:  96% 348/361 [05:02<00:11,  1.17it/s]\u001b[A\n",
            "Iteration:  97% 349/361 [05:02<00:10,  1.18it/s]\u001b[A\n",
            "Iteration:  97% 350/361 [05:03<00:09,  1.13it/s]\u001b[A\n",
            "Iteration:  97% 351/361 [05:04<00:08,  1.15it/s]\u001b[A\n",
            "Iteration:  98% 352/361 [05:05<00:07,  1.16it/s]\u001b[A\n",
            "Iteration:  98% 353/361 [05:06<00:06,  1.17it/s]\u001b[A\n",
            "Iteration:  98% 354/361 [05:07<00:05,  1.17it/s]\u001b[A\n",
            "Iteration:  98% 355/361 [05:08<00:05,  1.13it/s]\u001b[A\n",
            "Iteration:  99% 356/361 [05:09<00:04,  1.15it/s]\u001b[A\n",
            "Iteration:  99% 357/361 [05:09<00:03,  1.16it/s]\u001b[A\n",
            "Iteration:  99% 358/361 [05:10<00:02,  1.17it/s]\u001b[A\n",
            "Iteration:  99% 359/361 [05:11<00:01,  1.17it/s]\u001b[A\n",
            "Iteration: 100% 360/361 [05:12<00:00,  1.12it/s]\u001b[A\n",
            "Iteration: 100% 361/361 [05:13<00:00,  1.15it/s]\n",
            "02/27/2021 21:26:27 - INFO - __main__ -   Loading features from cached file /content/gpt2_cached_lm_512_star_wars_valid.txt\n",
            "02/27/2021 21:26:28 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/27/2021 21:26:28 - INFO - __main__ -     Num examples = 83\n",
            "02/27/2021 21:26:28 - INFO - __main__ -     Batch size = 2\n",
            "\n",
            "Evaluating:   0% 0/42 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:   2% 1/42 [00:00<00:10,  3.92it/s]\u001b[A\n",
            "Evaluating:   5% 2/42 [00:00<00:10,  3.80it/s]\u001b[A\n",
            "Evaluating:   7% 3/42 [00:00<00:10,  3.72it/s]\u001b[A\n",
            "Evaluating:  10% 4/42 [00:01<00:10,  3.60it/s]\u001b[A\n",
            "Evaluating:  12% 5/42 [00:01<00:10,  3.60it/s]\u001b[A\n",
            "Evaluating:  14% 6/42 [00:01<00:10,  3.57it/s]\u001b[A\n",
            "Evaluating:  17% 7/42 [00:01<00:10,  3.49it/s]\u001b[A\n",
            "Evaluating:  19% 8/42 [00:02<00:09,  3.51it/s]\u001b[A\n",
            "Evaluating:  21% 9/42 [00:02<00:09,  3.51it/s]\u001b[A\n",
            "Evaluating:  24% 10/42 [00:02<00:09,  3.48it/s]\u001b[A\n",
            "Evaluating:  26% 11/42 [00:03<00:08,  3.48it/s]\u001b[A\n",
            "Evaluating:  29% 12/42 [00:03<00:08,  3.50it/s]\u001b[A\n",
            "Evaluating:  31% 13/42 [00:03<00:08,  3.51it/s]\u001b[A\n",
            "Evaluating:  33% 14/42 [00:03<00:08,  3.47it/s]\u001b[A\n",
            "Evaluating:  36% 15/42 [00:04<00:07,  3.49it/s]\u001b[A\n",
            "Evaluating:  38% 16/42 [00:04<00:07,  3.50it/s]\u001b[A\n",
            "Evaluating:  40% 17/42 [00:04<00:07,  3.49it/s]\u001b[A\n",
            "Evaluating:  43% 18/42 [00:05<00:06,  3.48it/s]\u001b[A\n",
            "Evaluating:  45% 19/42 [00:05<00:06,  3.49it/s]\u001b[A\n",
            "Evaluating:  48% 20/42 [00:05<00:06,  3.50it/s]\u001b[A\n",
            "Evaluating:  50% 21/42 [00:05<00:06,  3.48it/s]\u001b[A\n",
            "Evaluating:  52% 22/42 [00:06<00:05,  3.49it/s]\u001b[A\n",
            "Evaluating:  55% 23/42 [00:06<00:05,  3.50it/s]\u001b[A\n",
            "Evaluating:  57% 24/42 [00:06<00:05,  3.49it/s]\u001b[A\n",
            "Evaluating:  60% 25/42 [00:07<00:04,  3.49it/s]\u001b[A\n",
            "Evaluating:  62% 26/42 [00:07<00:04,  3.50it/s]\u001b[A\n",
            "Evaluating:  64% 27/42 [00:07<00:04,  3.50it/s]\u001b[A\n",
            "Evaluating:  67% 28/42 [00:08<00:04,  3.47it/s]\u001b[A\n",
            "Evaluating:  69% 29/42 [00:08<00:03,  3.48it/s]\u001b[A\n",
            "Evaluating:  71% 30/42 [00:08<00:03,  3.49it/s]\u001b[A\n",
            "Evaluating:  74% 31/42 [00:08<00:03,  3.48it/s]\u001b[A\n",
            "Evaluating:  76% 32/42 [00:09<00:02,  3.48it/s]\u001b[A\n",
            "Evaluating:  79% 33/42 [00:09<00:02,  3.48it/s]\u001b[A\n",
            "Evaluating:  81% 34/42 [00:09<00:02,  3.48it/s]\u001b[A\n",
            "Evaluating:  83% 35/42 [00:10<00:02,  3.47it/s]\u001b[A\n",
            "Evaluating:  86% 36/42 [00:10<00:01,  3.47it/s]\u001b[A\n",
            "Evaluating:  88% 37/42 [00:10<00:01,  3.49it/s]\u001b[A\n",
            "Evaluating:  90% 38/42 [00:10<00:01,  3.49it/s]\u001b[A\n",
            "Evaluating:  93% 39/42 [00:11<00:00,  3.48it/s]\u001b[A\n",
            "Evaluating:  95% 40/42 [00:11<00:00,  3.49it/s]\u001b[A\n",
            "Evaluating:  98% 41/42 [00:11<00:00,  3.49it/s]\u001b[A\n",
            "Evaluating: 100% 42/42 [00:11<00:00,  3.53it/s]\n",
            "02/27/2021 21:26:39 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/27/2021 21:26:39 - INFO - __main__ -     perplexity = tensor(7.1037)\n",
            "02/27/2021 21:26:39 - INFO - __main__ -    global_step = 216, average loss = 1.9099979539988217\n",
            "Epoch: 100% 3/3 [16:15<00:00, 325.20s/it]\n",
            "02/27/2021 21:26:39 - INFO - __main__ -    global_step = 216, average loss = 1.9099979539988217\n",
            "02/27/2021 21:26:40 - INFO - __main__ -   Saving model checkpoint to /content/drive/My Drive/finetuned_models/star_wars_final\n",
            "02/27/2021 21:26:40 - INFO - transformers.configuration_utils -   Configuration saved in /content/drive/My Drive/finetuned_models/star_wars_final/config.json\n",
            "02/27/2021 21:27:05 - INFO - transformers.modeling_utils -   Model weights saved in /content/drive/My Drive/finetuned_models/star_wars_final/pytorch_model.bin\n",
            "02/27/2021 21:27:05 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/star_wars_final/config.json\n",
            "02/27/2021 21:27:05 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/27/2021 21:27:05 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/star_wars_final/pytorch_model.bin\n",
            "02/27/2021 21:28:03 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/star_wars_final/config.json\n",
            "02/27/2021 21:28:03 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/27/2021 21:28:03 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/finetuned_models/star_wars_final' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/finetuned_models/star_wars_final' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/27/2021 21:28:03 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/finetuned_models/star_wars_final/added_tokens.json. We won't load it.\n",
            "02/27/2021 21:28:03 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/star_wars_final/vocab.json\n",
            "02/27/2021 21:28:03 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/star_wars_final/merges.txt\n",
            "02/27/2021 21:28:03 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/27/2021 21:28:03 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/star_wars_final/special_tokens_map.json\n",
            "02/27/2021 21:28:03 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/star_wars_final/tokenizer_config.json\n",
            "02/27/2021 21:28:04 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/drive/My Drive/finetuned_models/star_wars_final']\n",
            "02/27/2021 21:28:04 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/star_wars_final/config.json\n",
            "02/27/2021 21:28:04 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/27/2021 21:28:04 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/star_wars_final/pytorch_model.bin\n",
            "02/27/2021 21:29:01 - INFO - __main__ -   Loading features from cached file /content/gpt2_cached_lm_512_star_wars_valid.txt\n",
            "02/27/2021 21:29:01 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "02/27/2021 21:29:01 - INFO - __main__ -     Num examples = 83\n",
            "02/27/2021 21:29:01 - INFO - __main__ -     Batch size = 2\n",
            "Evaluating: 100% 42/42 [00:12<00:00,  3.36it/s]\n",
            "02/27/2021 21:29:14 - INFO - __main__ -   ***** Eval results  *****\n",
            "02/27/2021 21:29:14 - INFO - __main__ -     perplexity = tensor(7.1037)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mAiosB2wBm"
      },
      "source": [
        "### Compute perplexity of a dataset.\n",
        "\n",
        "The following functions load a pretrained model either from the library or from a local directory. We can then compute the perplexity of a model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc2VCFBG3pFf"
      },
      "source": [
        "def load_model(args):\n",
        "  \"\"\"Creates a model and loads in weights for it.\"\"\"\n",
        "  config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "  #config = GPT2Config.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  model = AutoModelWithLMHead.from_pretrained(\n",
        "  #model = GPT2LMHeadModel.from_pretrained(\n",
        "      args.model_name_or_path,\n",
        "      \n",
        "  )\n",
        "  \n",
        "  model.to(args.device)\n",
        "       \n",
        "  \"\"\"model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "      from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "      config=config,\n",
        "      cache_dir=None\"\"\"\n",
        "\n",
        "    \n",
        "  return model\n",
        "\n",
        "def set_seed(args):\n",
        "  \"\"\"Set the random seed.\"\"\"\n",
        "  random.seed(args.seed)\n",
        "  np.random.seed(args.seed)\n",
        "  torch.manual_seed(args.seed)\n",
        "  if args.n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def do_perplexity_eval(args, model, data_file_path):\n",
        "  \"\"\"Computes the perplexity of the text in data_file_path according to the provided model.\"\"\"\n",
        "  set_seed(args)\n",
        "\n",
        "  args.eval_data_file=data_file_path\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "\n",
        "  result = run_language_modeling.evaluate(args, model, tokenizer, prefix=\"\")\n",
        "  return result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERCKSncEBYgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410d8c77-9f77-4a47-f465-ab3bac1d7271"
      },
      "source": [
        "# Set this to the checkpoint you want to evalute, or to \"gpt2-medium\" to\n",
        "# evaluate the pre-trained model without finetuning.\n",
        "CHECKPOINT_PATH = '/content/drive/MyDrive/finetuned_models/star_wars_final'\n",
        "#CHECKPOINT_PATH = \"gpt2-medium\"\n",
        "\n",
        "# Set this to the list of text files you want to evaluate the perplexity of.\n",
        "DATA_PATHS = [\"/content/star_wars_valid.txt\",\n",
        "              \"/content/star_wars_test.txt\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=CHECKPOINT_PATH,\n",
        "  block_size = 512,\n",
        "  local_rank=-1,\n",
        "  eval_batch_size=2,\n",
        "  per_gpu_eval_batch_size=2,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  line_by_line=False,\n",
        "  overwrite_cache=None,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        ")\n",
        "#args = DictToObj(args)\n",
        "args = to_object(args)\n",
        "\n",
        "model = load_model(args)\n",
        "\n",
        "for data_path in DATA_PATHS:\n",
        "  eval_results = do_perplexity_eval(args, model, data_path)\n",
        "  perplexity = eval_results['perplexity']\n",
        "  print('{} is the perplexity of {} according to {}'.format(\n",
        "      perplexity, data_path, CHECKPOINT_PATH))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/28/2021 11:34:45 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 11:34:45 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 11:34:45 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 11:34:45 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 11:34:45 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/finetuned_models/star_wars_final/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on device:  cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/28/2021 11:35:02 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 11:35:02 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 11:35:02 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/finetuned_models/star_wars_final' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/MyDrive/finetuned_models/star_wars_final' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/28/2021 11:35:02 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/MyDrive/finetuned_models/star_wars_final/added_tokens.json. We won't load it.\n",
            "02/28/2021 11:35:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/vocab.json\n",
            "02/28/2021 11:35:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/merges.txt\n",
            "02/28/2021 11:35:02 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/28/2021 11:35:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/special_tokens_map.json\n",
            "02/28/2021 11:35:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/tokenizer_config.json\n",
            "02/28/2021 11:35:02 - INFO - run_language_modeling -   Creating features from dataset file at /content\n",
            "02/28/2021 11:35:03 - INFO - run_language_modeling -   Saving features into cached file /content/gpt2_cached_lm_512_star_wars_valid.txt\n",
            "02/28/2021 11:35:03 - INFO - run_language_modeling -   ***** Running evaluation  *****\n",
            "02/28/2021 11:35:03 - INFO - run_language_modeling -     Num examples = 83\n",
            "02/28/2021 11:35:03 - INFO - run_language_modeling -     Batch size = 2\n",
            "Evaluating: 100%|██████████| 42/42 [00:20<00:00,  2.03it/s]\n",
            "02/28/2021 11:35:23 - INFO - run_language_modeling -   ***** Eval results  *****\n",
            "02/28/2021 11:35:23 - INFO - run_language_modeling -     perplexity = tensor(7.1037)\n",
            "02/28/2021 11:35:23 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 11:35:23 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 11:35:23 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/finetuned_models/star_wars_final' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/MyDrive/finetuned_models/star_wars_final' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/28/2021 11:35:23 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/MyDrive/finetuned_models/star_wars_final/added_tokens.json. We won't load it.\n",
            "02/28/2021 11:35:23 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/vocab.json\n",
            "02/28/2021 11:35:23 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/merges.txt\n",
            "02/28/2021 11:35:23 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/28/2021 11:35:23 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/special_tokens_map.json\n",
            "02/28/2021 11:35:23 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/tokenizer_config.json\n",
            "02/28/2021 11:35:23 - INFO - run_language_modeling -   Creating features from dataset file at /content\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7.103740692138672 is the perplexity of /content/star_wars_valid.txt according to /content/drive/MyDrive/finetuned_models/star_wars_final\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/28/2021 11:35:24 - INFO - run_language_modeling -   Saving features into cached file /content/gpt2_cached_lm_512_star_wars_test.txt\n",
            "02/28/2021 11:35:24 - INFO - run_language_modeling -   ***** Running evaluation  *****\n",
            "02/28/2021 11:35:24 - INFO - run_language_modeling -     Num examples = 103\n",
            "02/28/2021 11:35:24 - INFO - run_language_modeling -     Batch size = 2\n",
            "Evaluating: 100%|██████████| 52/52 [00:25<00:00,  2.03it/s]\n",
            "02/28/2021 11:35:49 - INFO - run_language_modeling -   ***** Eval results  *****\n",
            "02/28/2021 11:35:49 - INFO - run_language_modeling -     perplexity = tensor(7.6790)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7.679045677185059 is the perplexity of /content/star_wars_test.txt according to /content/drive/MyDrive/finetuned_models/star_wars_final\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5o7v2hmhMTO"
      },
      "source": [
        "### Generate samples\n",
        "The following code generates text samples that are are continuations of a provided prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcvySe_wrCWh"
      },
      "source": [
        "def generate_samples(args, model, prompt_text):\n",
        "  \"\"\"Generating sampling for the provided prompt using the provided model.\"\"\"\n",
        "\n",
        "  print(args)\n",
        "  set_seed(args)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  requires_preprocessing = args.model_type in run_generation.PREPROCESSING_FUNCTIONS.keys()\n",
        "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=args.length + len(encoded_prompt[0]),\n",
        "      temperature=args.temperature,\n",
        "      top_k=args.k,\n",
        "      top_p=args.p,\n",
        "      repetition_penalty=args.repetition_penalty,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=args.num_return_sequences,\n",
        "  )\n",
        "\n",
        "  # Remove the batch dimension when returning multiple sequences\n",
        "  if len(output_sequences.shape) > 2:\n",
        "    output_sequences.squeeze_()\n",
        "\n",
        "  generated_sequences = []\n",
        "\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "    # Decode text\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Remove all text after the stop token\n",
        "    text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "    # Remove the excess text that was used for pre-processing\n",
        "    text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "    # Add the prompt at the beginning of the sequence.\n",
        "    total_sequence = prompt_text + text\n",
        "\n",
        "    generated_sequences.append(total_sequence)\n",
        "\n",
        "  return generated_sequences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3LKo9VVjHw0"
      },
      "source": [
        "# Generation of original text from a prompt \n",
        "\n",
        "def generate_output(model_path, prompt):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    #print(\"Running on device: \", device)\n",
        "\n",
        "    CHECKPOINT_PATH = model_path\n",
        "    PROMPT = prompt\n",
        "\n",
        "    args = collections.defaultdict(\n",
        "      model_name_or_path=CHECKPOINT_PATH,\n",
        "      output_dir=CHECKPOINT_PATH,\n",
        "      n_gpu=n_gpu,\n",
        "      mlm=False,\n",
        "      device=device,\n",
        "      model_type='gpt2',\n",
        "      seed=42,\n",
        "      stop_token=None, # Set this if your dataset has a special word that indicates the end of a text.\n",
        "      temperature=1.0,  # temperature sampling. Set this to temperature=1.0 to not use temperature.\n",
        "      k=50,  # k for top-k sampling. Set this to k=0 to not use top-k.\n",
        "      p=0.9,  # p for nucleus sampling. Set this to p=1.0 to not use nucleus sampling.\n",
        "      repetition_penalty= 1.1,\n",
        "      length=100,  # Number of tokens to generate.\n",
        "      num_return_sequences=10,  # Number of independently computed samples to generate.\n",
        "    )\n",
        "    #args = DictToObj(args)\n",
        "    args = to_object(args)\n",
        "\n",
        "    model = load_model(args)\n",
        "    print(args)\n",
        "    sequences = generate_samples(args, model, PROMPT)\n",
        "    for idx, sequence in enumerate(sequences):\n",
        "      print('\\n====== GENERATION {} ======'.format(idx))\n",
        "      print(sequence)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5IutohKmQW7",
        "outputId": "c8472780-34c5-4b54-e3ee-54deb1e7478d"
      },
      "source": [
        "# Set this to the checkpoint you want to use for generation, or to \"gpt2-medium\"\r\n",
        "# to generate with the pre-trained model without finetuning.\r\n",
        "CHECKPOINT_PATH = '/content/drive/MyDrive/finetuned_models/star_wars_final/'\r\n",
        "\r\n",
        "PROMPT = 'Leia and Luke look to the sky, as a spaceship approaches.'\r\n",
        "\r\n",
        "#generate_output( \"gpt2-medium\", PROMPT)\r\n",
        "generate_output(CHECKPOINT_PATH, PROMPT)\r\n",
        "\r\n",
        "PROMPT2 = \"A long time ago, in a galaxy far, far away...\"\r\n",
        "\r\n",
        "generate_output(CHECKPOINT_PATH, PROMPT2)\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/28/2021 13:43:37 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 13:43:37 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 13:43:37 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 13:43:37 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 13:43:37 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/finetuned_models/star_wars_final/pytorch_model.bin\n",
            "02/28/2021 13:44:25 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 13:44:25 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 13:44:25 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/finetuned_models/star_wars_final/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/MyDrive/finetuned_models/star_wars_final/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/28/2021 13:44:25 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/MyDrive/finetuned_models/star_wars_final/added_tokens.json. We won't load it.\n",
            "02/28/2021 13:44:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/vocab.json\n",
            "02/28/2021 13:44:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/merges.txt\n",
            "02/28/2021 13:44:25 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/28/2021 13:44:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/special_tokens_map.json\n",
            "02/28/2021 13:44:25 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/tokenizer_config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.jo'>\n",
            "<class '__main__.jo'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/28/2021 13:44:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
            "02/28/2021 13:44:33 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 13:44:33 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 13:44:33 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 13:44:33 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 13:44:33 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/finetuned_models/star_wars_final/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====== GENERATION 0 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches.\n",
            "     The ship lifts off like a rocket toward their location in space. Leia looks over at Han; he's also looking away from the craft.\n",
            "HAN : (Cont'd) Where are we going?\n",
            "LEIA : We're just passing through there -- I guess this is it! Look... where am??\n",
            "THREEPIO laughs nervously. He puts his hand on an air filter that washes out the spray of blood left by Lando's decapitated head\n",
            "\n",
            "====== GENERATION 1 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches.\n",
            "LEIA: What's it over there?\n",
            "PADMÉ\n",
            "There's no way that could have happened... we're in trouble!  I'm sending you two back before this whole thing blows up!\n",
            "EXTERIOR -- HOTH -- DARTH VADER'S CASTLE -- MAIN BRIDGE\n",
            "Vader leads Poe down a long hallway where several guards guard droids are waiting for him. He stops suddenly on an upper level with his eyes dart\n",
            "\n",
            "====== GENERATION 2 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches.  The ship's\n",
            "light is very soft on their faces as they hold each other tight in an embrace.\n",
            "INT. LUKE'S X-WING - COCKPIT\n",
            "Luke lets out a terrified breath at what he sees ahead.  He stares into his canopy viewfinder where Ben Kenobi has placed him next to Leia in her Falcon cockpit.  Suddenly something knocks against the side of Luke's starship: two TIE fighters attack it from all sides.  A laser bolt pier\n",
            "\n",
            "====== GENERATION 3 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches.\n",
            "         The cockpit lights are lit with red fire on a white board\n",
            "       attached to one of the ship's wings. There are two controllers sitting nearby.\n",
            "WEDGE: You're going in?\n",
            "LEIA (to Chewie): Get ready! Bring out your laser cannon. We've got to go fast. It can't be by accident. And now we need you to stay close! No more talking... wait\n",
            "\n",
            "====== GENERATION 4 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches.\n",
            "INT. DEATH STAR - CONTROL ROOM\n",
            "The door slides open with an electronic buzz and lights turn on in a blur around them.  Darth Vader walks up to the control panel where his aide is waiting anxiously.\n",
            "VADER\n",
            "Ahhh...very good morning, Your Highness.\n",
            "INTERIOR: MILLENNIUM FALCON -- COCKPIT\n",
            "\n",
            "        The Falcon makes its final approach to the Death Star trench at top speed\n",
            "\n",
            "====== GENERATION 5 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches.\n",
            "        The two men turn off their power lights. Their ship is just overhead\n",
            "        in blackness. At first they don't notice it -- but then Leia's head appears on\n",
            "        her shoulder beam, like she's trying not be noticed. Her blue hair\n",
            "is tousled back with bangs; there is no doubt about who or what she really was. She can be\n",
            "  \n",
            "\n",
            "====== GENERATION 6 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches.\n",
            "     INT DEATH STAR - CONFERENCE ROOM                     HAN\n",
            "         I've got it! They'll be back in about six minutes.\n",
            "EXT. SPACE AROUND THE MOON\n",
            "Two B1 battle droids move on toward the Death Star surface. The speeder drops off at an intersection with two Imperial TIE fighters. Two\n",
            "\n",
            "====== GENERATION 7 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. A strange shape appears in the horizon's cloud cover and\n",
            "     moves into view -- its outline impossible for anyone else from now on. The ship zooms by! There is no sound... only darkness. In an instant all of it disappears. No sign of life.... or anything except... some kind not unlike smoke. Then suddenly two shapes disappear just before their point of intersection: They are moving very fast!!! Suddenly they merge with one another like twin moons orbiting around each other -- until\n",
            "\n",
            "====== GENERATION 8 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches.\n",
            "          The ship is in sight; it's just ahead. A large FLEET of PLANEShips moves past us toward where this huge Star Destroyer will land.\n",
            "INTERIOR: STAR DESTROYER -- COCKPIT\n",
            "Luke looks at his speeder control panel with concern, then he pushes an increase button on his pod computer. Suddenly:\n",
            "EXT'S BRIDGE - ASTEROIDS HALLWAY - DAY astroids\n",
            "\n",
            "====== GENERATION 9 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches.\n",
            "INT. LUKE'S X-WING FIGHTER - COCKPIT\n",
            "\n",
            "     A distant voice calls out from outside the cockpit window. Several other voices are heard on the inside of the fighter as Threepio sits up in bed. He is dressed for his flight training session but comes away without taking off his headset. The wind howls around him as he works something over on his computer monitor. Lando's hand rises gently toward Leia; they stand\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/28/2021 13:45:02 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 13:45:02 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 13:45:02 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/finetuned_models/star_wars_final/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/MyDrive/finetuned_models/star_wars_final/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/28/2021 13:45:02 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/MyDrive/finetuned_models/star_wars_final/added_tokens.json. We won't load it.\n",
            "02/28/2021 13:45:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/vocab.json\n",
            "02/28/2021 13:45:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/merges.txt\n",
            "02/28/2021 13:45:02 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/28/2021 13:45:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/special_tokens_map.json\n",
            "02/28/2021 13:45:02 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/tokenizer_config.json\n",
            "02/28/2021 13:45:02 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.jo'>\n",
            "<class '__main__.jo'>\n",
            "\n",
            "====== GENERATION 0 ======\n",
            "A long time ago, in a galaxy far, far away...\n",
            "PALPATINE (in hologram) If I may ask of you something personal. And it's important.... It might change your destiny. But be careful what is written in the dark side of the Force. You cannot keep yourself from my gaze forever. When times are difficult, when danger is threatening to everyone you love... take heart. There will always come a day when you must learn to trust me. Trust only that which pleases me. Protecting someone who does not deserve\n",
            "\n",
            "====== GENERATION 1 ======\n",
            "A long time ago, in a galaxy far, far away...\n",
            "LEIA:...My father once gave us this message to you. I hope we can return it safely home.\"\n",
            "EXT-STARKILLER BASE - DAY\n",
            "GENERAL GRIEVOUS stands and surveys the battle scene around him with some concern. THREEPIO is sitting by Leia's side -- not looking toward her at all.  He gets up slowly as he sees General Hux standing on the observation platform overlooking the trench of the Death Star.  His eyes light up\n",
            "\n",
            "====== GENERATION 2 ======\n",
            "A long time ago, in a galaxy far, far away...\n",
            "        The First Order has entered the capital of Alderaan. Organa and friends are fleeing across the desolate desert floor below. When Leia finds her old friend on the topmost level, she is shaken to her core. She stares at him with tears in their eyes. He waves frantically for help. But when he turns around, there are no more bodies nor any sign that anyone has gone missing. Suddenly, Artoo appears -- it is Obi-Wan Kenobi\n",
            "\n",
            "====== GENERATION 3 ======\n",
            "A long time ago, in a galaxy far, far away...\n",
            "PADME: Where are they taking him? We don't know. He's not leaving the Jedi Temple like we're asking. That would be suicide! The Council is divided on this. If he goes underground then it means war; and I doubt if Obi-Wan will continue to stay with the Queen.... but what about you? Are there any more of your kind left here? It sounds as though everything has fallen apart now for Luke Skywalker. You've been reduced by circumstance to an\n",
            "\n",
            "====== GENERATION 4 ======\n",
            "A long time ago, in a galaxy far, far away...\n",
            "(CONTINUED)\n",
            "\n",
            "        The battle rages throughout the ship. Threepio works on his controls as Chewie barks commands at him. He sees Artoo-Detoo beeping frantically into the comlink system. A hologram of himself appears next to Han and Luke and they head for their cockpit exits. Leia looks worried about her little droid. Her face is very serious. It would appear that she might suffer another stroke or\n",
            "\n",
            "====== GENERATION 5 ======\n",
            "A long time ago, in a galaxy far, far away...\n",
            "PADME and the WOOKIEE watch as ANAKIN takes off on his X-wing fighter. PADDLE'S COCKPIT is filled with computer chips for various weapons systems. ARTOO chirps animatedly to each of them. Suddenly all PANELS DISAPPEAR. The TIE FIGHTERS WHINE -- they are not supposed this close to takeoff! They keep sliding backwards until everything except one STOPS FLYING IN FRAME - IT IS A\n",
            "\n",
            "====== GENERATION 6 ======\n",
            "A long time ago, in a galaxy far, far away...\n",
            "          Anakin Skywalker takes his ship out of hyperspace and races toward the battle.\n",
            "EXT TIE Fighter LANDING PLATFORM\n",
            "The fighter streaks through the sky as it moves to within about fifty feet off the ground. Two X-wings begin circling overhead. Other XIMO fighters land nearby. The BATTLE DROID is being carried by several HOLOGRAMS who are trying not be overwhelmed. They don't know they have\n",
            "\n",
            "====== GENERATION 7 ======\n",
            "A long time ago, in a galaxy far, far away... A friend introduced me to the Force.\n",
            "      My memory of it is foggy. I cannot describe how my heart skipped a beat when Obi-Wan first gave me\n",
            "     this power.... But then suddenly you were there for me, and they vanished from\n",
            "undisputed history! You have restored peace and justice to our war torn planet. And now?\n",
            "     This young Jedi has brought hope to so many who know little or nothing about\n",
            "\n",
            "====== GENERATION 8 ======\n",
            "A long time ago, in a galaxy far, far away...\n",
            "The FOUR HOLOGRAMS of NABOO PRIME MINISTER DOOKU are projected on the back of ARTILLERY SPEEDERS. He looks over at OBI-WAN who has been carefully sitting with him as they chat about their plan to destroy the Separatist leaders. DARTH MAUL sits next JAR JAR and is talking into his comlink. MACE WINDU speaks for THREEPIO's voiceover:  INT BASE CLOS\n",
            "\n",
            "====== GENERATION 9 ======\n",
            "A long time ago, in a galaxy far, far away...\n",
            "ARTOO whistles his concern. The Wookiee looks up and smiles sadly.\n",
            "ANAKIN :...there's nothing to see around here anymore. It was so peaceful! We were always alone together. And now it's over. I miss you. You made me realize the beauty of life. Now we're all alone again. My feelings for you are not going anywhere, Chewie -- your thoughts have been my reality since I saw you four years before. Your presence brings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zclapdrbu5Bq",
        "outputId": "db142803-9545-4a89-cd45-19bfcec8f78f"
      },
      "source": [
        "PROMPT3 = 'Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \\n BEN: Luke, Leia, I come in peace. Please allow me access to the base. \\n LEIA:'\r\n",
        "\r\n",
        "generate_output(CHECKPOINT_PATH, PROMPT3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/28/2021 14:03:12 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 14:03:12 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 14:03:12 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 14:03:12 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 14:03:12 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/finetuned_models/star_wars_final/pytorch_model.bin\n",
            "02/28/2021 14:03:26 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/28/2021 14:03:26 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/28/2021 14:03:26 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/finetuned_models/star_wars_final/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/MyDrive/finetuned_models/star_wars_final/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/28/2021 14:03:26 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/MyDrive/finetuned_models/star_wars_final/added_tokens.json. We won't load it.\n",
            "02/28/2021 14:03:26 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/vocab.json\n",
            "02/28/2021 14:03:26 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/merges.txt\n",
            "02/28/2021 14:03:26 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/28/2021 14:03:26 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/special_tokens_map.json\n",
            "02/28/2021 14:03:26 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/tokenizer_config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.jo'>\n",
            "<class '__main__.jo'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/28/2021 14:03:26 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====== GENERATION 0 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: What?!? Ben! The Queen asked you to go see her personally. To discuss this with our new Supreme Leader. So here we are now...and only then can she know how much your friendship extends. Let me explain...\n",
            "PALPATINE\n",
            "(continuing) No one will be able pinpoint where Skywalker left that trail until he returns alive. Only those who have followed him may do so, but they must stay hidden for their own protection. And please find an old friend of\n",
            "\n",
            "====== GENERATION 1 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: This isn't good. It seems our friend Han has been captured by the evil Empire.\n",
            "THREEPIO\n",
            "(continuing) There's nothing you can do about this! We've got the whole galaxy on our radar! You want him back? He'll be made into an Imperial slave.., not part of your mission! Yours was only training! That won't save his life now that he's aboard! Go get her! Take all possible measures against the\n",
            "\n",
            "====== GENERATION 2 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: Sir, there are so many secrets about this location you have no idea where to begin.  You'll need all of our abilities. All day long... go help us with something!\n",
            "Ben turns away toward his office, followed by YODA. LUKE stares at him blankly. Then he takes off through space into the deep blue horizon.\n",
            "INT. MILLENNIUM FALCON - COCKPIT\n",
            "\n",
            "Luke has been carefully planning out every last move that will save himself\n",
            "\n",
            "====== GENERATION 3 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: General Solo has no power here...no troops....only men with weapons.\n",
            "ANAKIN puts his hand on Chewie's shoulder. The Wookiee begins to cry. Ben looks at him again.\n",
            "[Woke talking]\n",
            "BEN: Han will have enough of this nonsense when he gets back. So go get your father! Get yourself outta there fast, buddy! Goodnight!\n",
            "HAN (to droid) Hey, Artoo! No more waiting for you\n",
            "\n",
            "====== GENERATION 4 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: Thank you, Master Ben. (applause) Chewie... we were worried when our scouts reported seeing someone out there.  Do you know who they were?\n",
            "Ben turns away his head toward Princess Rey; she seems bewildered by this sudden turn of events. She reaches for something on her belt -- then realizes what has happened and walks back up the corridor where Kylo Ren is hiding. He hears Han say that he will not take any prisoners with him. Now was a great\n",
            "\n",
            "====== GENERATION 5 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: The Millennium Falcon belongs on this planet. This way.\n",
            "BEN: We are very sorry about your father's death. Your mother will take care of him now... for you.\n",
            "LEAA: (softly) Ohhh.... but can we just say goodbye?\n",
            "Luke glances back at his uncle with sympathy. He puts some ice over her shoulder. She looks away nervously.\n",
            "\n",
            "        Ben takes off into space alongside Threepio against the backdrop star\n",
            "\n",
            "====== GENERATION 6 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: Thank you, Ben.\n",
            "INT. THEED - REBEL BASE - DAY (FX)\n",
            "The group enters Theed's main entrance on one of the long catwalks into the massive facility. OBI-WAN steps up onto one platform toward two X-wing fighters that are parked above him. ARTOO pops out behind his friend BIB DROID FETT who has brought JAR JAR down with them. Both BBs have lasers aimed at the Jedi Leader\n",
            "\n",
            "====== GENERATION 7 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: Aaak! Hold on. This way. Chewie... Artoo's here too. Go for him quickly.\n",
            "They head up a long corridor of cells like droids at the back of an assembly line. Ben notices two guards outside one cell. He lets go briefly of Rey when he hears something unusual. When they reach another side cell -- this one not so far from where Skywalker lived with his Aunt Beru -- ANAKIN enters out the door behind them. She\n",
            "\n",
            "====== GENERATION 8 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: (pointing to Han) You think you can trust that old fool?  This place isn't what it used be.\n",
            "INT. CORUSCANT-AMIDALA'S APARTMENT-ROOM - NIGHT\n",
            "The room is deserted except for ANAKIN standing on her balcony with THREEPIO perched next door. She watches him lovingly through his goggles as they walk away. He leans down near Threepio's face and kisses him softly on both cheeks\n",
            "\n",
            "====== GENERATION 9 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. They cannot yet determine if it is a ship from the Resistance, or the First Order. \n",
            " BEN: Luke, Leia, I come in peace. Please allow me access to the base. \n",
            " LEIA: It's not my fault she doesn't know how basic that thing is.  You're on your own with all this...  We can keep her safe here for an indefinite time. But you better make sure we don't get too far away before someone takes us out of range.\n",
            "BEN: If anything does happen to either of us, be ready. And give up any plans?\n",
            "LEIE: Oh, Ben, no way! Our mission was simple.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "27mof7PlgLlm",
        "outputId": "a516aeb6-0918-406a-8c76-c13c9bdea83c"
      },
      "source": [
        "PROMPT5 = \"A long time ago, in a galaxy far, far away... The Galaxy is finally at peace, after the defeat of the First Order. Rey Skywalker is training a new generation of Jedi Knights, at the Academy on Tatooine. But the shadows of old enemies\"\r\n",
        "generate_output(CHECKPOINT_PATH, PROMPT5)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a3e773c9ecff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPROMPT5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"A long time ago, in a galaxy far, far away... The Galaxy is finally at peace, after the defeat of the First Order. Rey Skywalker is training a new generation of Jedi Knights, at the Academy on Tatooine. But the shadows of old enemies\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROMPT5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_output' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFeMHjelwQgQ",
        "outputId": "a8f3d284-e7c4-455b-aa28-eec8f6830806"
      },
      "source": [
        "PROMPT4 = \"Leia and Luke look to the sky, as a spaceship approaches. \\n LUKE \\n (over speaker) \\n We've spotted that spaceship. \\n Luke looks at the small yellow dot which the ship has crossed over, and thinks it through for a moment. He then puts his head in his hands.\"\r\n",
        "generate_output(CHECKPOINT_PATH, PROMPT4)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/27/2021 22:12:57 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/27/2021 22:12:57 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/27/2021 22:12:57 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/27/2021 22:12:57 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/27/2021 22:12:57 - INFO - transformers.modeling_utils -   loading weights file /content/drive/MyDrive/finetuned_models/star_wars_final/pytorch_model.bin\n",
            "02/27/2021 22:13:54 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/MyDrive/finetuned_models/star_wars_final/config.json\n",
            "02/27/2021 22:13:54 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "02/27/2021 22:13:54 - INFO - transformers.tokenization_utils -   Model name '/content/drive/MyDrive/finetuned_models/star_wars_final/' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/MyDrive/finetuned_models/star_wars_final/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "02/27/2021 22:13:54 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/MyDrive/finetuned_models/star_wars_final/added_tokens.json. We won't load it.\n",
            "02/27/2021 22:13:54 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/vocab.json\n",
            "02/27/2021 22:13:54 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/merges.txt\n",
            "02/27/2021 22:13:54 - INFO - transformers.tokenization_utils -   loading file None\n",
            "02/27/2021 22:13:54 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/special_tokens_map.json\n",
            "02/27/2021 22:13:54 - INFO - transformers.tokenization_utils -   loading file /content/drive/MyDrive/finetuned_models/star_wars_final/tokenizer_config.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.jo'>\n",
            "<class '__main__.jo'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "02/27/2021 22:13:55 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====== GENERATION 0 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. \n",
            " LUKE \n",
            " (over speaker) \n",
            " We've spotted that spaceship. \n",
            " Luke looks at the small yellow dot which the ship has crossed over, and thinks it through for a moment. He then puts his head in his hands.\n",
            "INT. MILLENNIUM FALCON - COCKPIT/SPACE\n",
            "The Falcon sits on a large hangar floor surrounded by rows of tiny little craft. The two Millennium Falcon pilots chat away while several PILOTS pass out food from some hanging lids -- ANAKIN graciously does so. LANDO lets go with an awkward shake of Artoo's neck.\n",
            "LANDo turns back around and gives Han Solo another hug.\n",
            "LARK: So what brings you\n",
            "\n",
            "====== GENERATION 1 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. \n",
            " LUKE \n",
            " (over speaker) \n",
            " We've spotted that spaceship. \n",
            " Luke looks at the small yellow dot which the ship has crossed over, and thinks it through for a moment. He then puts his head in his hands.\n",
            "LUCE: No no no! What are you doing here?\n",
            "INT. CORUSCANT - MAIN HANGAR DECK - NIGHT cloaks pass with ease across the ice cap.  The starship moves quickly out of sight on the ramp leading to the hangar deck.\n",
            "EXT'L SPACE CLOUD SHIP\n",
            "Luke races down the elevator shaft into the cramped hold area behind the Starship Avenger. A trooper comes around one of those giant sliding doors.\n",
            "R2-\n",
            "\n",
            "====== GENERATION 2 ======\n",
            "Leia and Luke look to the sky, as a spaceship approaches. \n",
            " LUKE \n",
            " (over speaker) \n",
            " We've spotted that spaceship. \n",
            " Luke looks at the small yellow dot which the ship has crossed over, and thinks it through for a moment. He then puts his head in his hands. \n",
            "EXT'S MOON FARM - BATTLEFIELD - NIGHTLANDING PLATFORM\n",
            "The TIE fighter streaks overhead toward Lando's farm. It is flying near him on one of the windscreens.\n",
            "INT. LANDO'C HOMESTEAD - COCKPIT\n",
            "Landino steps off the Falcon into the cramped little cockpit. The view switches between stars... distant objects moving fast across the screen. Suddenly Leia leans forward from behind her pilot\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}